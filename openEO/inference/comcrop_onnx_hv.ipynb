{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c277071",
   "metadata": {},
   "source": [
    "## Define Area of Interest (AOI)\n",
    "\n",
    "Use the map below to draw a rectangle defining your processing area. The maximum size is approximately 600x600 pixels at 10m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9787d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import openeo\n",
    "from extractor import load_input_WAC\n",
    "from inference import inference_WAC\n",
    "\n",
    "\n",
    "CRS = \"EPSG:3035\"\n",
    "RESOLUTION = 10  # important; the resolution is implicitely tied to the CRS; so we need to use UTM based CRS here\n",
    "PATCH_SIZE = 64\n",
    "OVERLAP_SIZE = 0\n",
    "\n",
    "SPATIAL_EXTENT = {\n",
    "    'west': 300000,\n",
    "    'south': 9800000,\n",
    "    'east': 305000,  \n",
    "    'north': 9805000,  \n",
    "    'crs': CRS  \n",
    "}\n",
    "\n",
    "N_CLASSES = 21\n",
    "\n",
    "TEMPORAL_EXTENT = ['2023-06-01', '2023-08-01'] \n",
    "MAX_CLOUD_COVER = 85\n",
    "\n",
    "\n",
    "JOB_OPTIONS = {'driver-memory': '2000m',\n",
    " 'driver-memoryOverhead': '2000m',\n",
    " 'executor-memory': '3000m',\n",
    " 'executor-memoryOverhead': '3000m',\n",
    " 'python-memory': '8000m',\n",
    " 'max-executors': 20,\n",
    " \"udf-dependency-archives\": [\n",
    "        \"https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/onnx_dependencies_1.16.3.zip#onnx_deps\",\n",
    "        \"https://s3.waw3-1.cloudferro.com/swift/v1/project_dependencies/FusionUNet_OilpalmModel_20250718131837.zip#onnx_models\"\n",
    "        ]\n",
    " }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2c87a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authenticated using refresh token.\n",
      "0:00:00 Job 'j-2507181142564c2c914eccd469bc2c06': send 'start'\n",
      "0:00:12 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:00:17 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:00:24 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:00:32 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:00:42 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:00:54 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:01:10 Job 'j-2507181142564c2c914eccd469bc2c06': queued (progress 0%)\n",
      "0:01:29 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:01:53 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:02:23 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:03:01 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:03:47 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:04:46 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:05:46 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:06:46 Job 'j-2507181142564c2c914eccd469bc2c06': running (progress N/A)\n",
      "0:07:46 Job 'j-2507181142564c2c914eccd469bc2c06': finished (progress 100%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-job')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-job>\n",
       "        <script type=\"application/json\">{\"currency\": \"credits\", \"job\": {\"costs\": 10, \"created\": \"2025-07-18T11:42:56Z\", \"id\": \"j-2507181142564c2c914eccd469bc2c06\", \"process\": {\"process_graph\": {\"adddimension1\": {\"arguments\": {\"data\": {\"from_node\": \"ndvi1\"}, \"label\": \"NDVI\", \"name\": \"bands\", \"type\": \"bands\"}, \"process_id\": \"add_dimension\"}, \"aggregatetemporalperiod1\": {\"arguments\": {\"data\": {\"from_node\": \"mask1\"}, \"period\": \"month\", \"reducer\": {\"process_graph\": {\"mean1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\", \"result\": true}}}}, \"process_id\": \"aggregate_temporal_period\"}, \"apply1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial1\"}, \"process\": {\"process_graph\": {\"log1\": {\"arguments\": {\"base\": 10, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"log\"}, \"multiply1\": {\"arguments\": {\"x\": 10, \"y\": {\"from_node\": \"log1\"}}, \"process_id\": \"multiply\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"apply2\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod1\"}, \"process\": {\"process_graph\": {\"runudf1\": {\"arguments\": {\"context\": {\"crs\": \"EPSG:3035\", \"east\": 305000, \"north\": 9805000, \"south\": 9800000, \"west\": 300000}, \"data\": {\"from_parameter\": \"x\"}, \"runtime\": \"Python\", \"udf\": \"import numpy as np\\nimport xarray as xr\\nimport logging\\nfrom pyproj import Transformer\\nfrom typing import Dict\\n\\n# Setup logging\\ndef _setup_logging() -> logging.Logger:\\n    logging.basicConfig(level=logging.INFO, format=\\\"%(message)s\\\")\\n    return logging.getLogger(__name__)\\n\\nlogger = _setup_logging()\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Constructs a lon/lat grid as a new DataArray based on the cube's spatial resolution\\n    and the geographic extent provided in `context`.\\n\\n    Args:\\n        cube (xr.DataArray): Input data cube with 'x' and 'y' dimensions.\\n        context (dict): Dictionary containing 'west', 'south', 'east', 'north', and 'crs'.\\n\\n    Returns:\\n        xr.DataArray: A new DataArray of shape (2, y, x) with bands ['lon', 'lat'].\\n    \\\"\\\"\\\"\\n\\n    # Parse extent and CRS\\n    try:\\n        west  = float(context[\\\"west\\\"])\\n        south = float(context[\\\"south\\\"])\\n        east  = float(context[\\\"east\\\"])\\n        north = float(context[\\\"north\\\"])\\n        crs   = context[\\\"crs\\\"]\\n    except KeyError as e:\\n        raise ValueError(f\\\"Missing required context key: {e}\\\")\\n\\n    logger.info(f\\\"Original extent: {west}, {south} \\u2192 {east}, {north} in {crs}\\\")\\n\\n    # Transform extent to EPSG:4326 if needed\\n    if crs != \\\"EPSG:4326\\\":\\n        transformer = Transformer.from_crs(crs, \\\"EPSG:4326\\\", always_xy=True)\\n        west, south = transformer.transform(west, south)\\n        east, north = transformer.transform(east, north)\\n        logger.info(f\\\"Transformed extent to EPSG:4326: {west}, {south} \\u2192 {east}, {north}\\\")\\n\\n    # Get cube dimensions\\n    nx = cube.sizes[\\\"x\\\"]\\n    ny = cube.sizes[\\\"y\\\"]\\n\\n    # Create lon/lat coordinate arrays\\n    lon = np.linspace(west, east, nx, dtype=np.float32)\\n    lat = np.linspace(north, south, ny, dtype=np.float32)  # north \\u2192 south to match image orientation\\n\\n    # Generate 2D meshgrid\\n    lon_grid, lat_grid = np.meshgrid(lon, lat)\\n\\n    logger.info(f\\\"Longitude range: {lon_grid.min()} to {lon_grid.max()}\\\")\\n    logger.info(f\\\"Latitude range: {lat_grid.min()} to {lat_grid.max()}\\\")\\n\\n    # Build output DataArray\\n    return xr.DataArray(\\n        data=np.stack([lon_grid, lat_grid], axis=0),  # shape: (2, y, x)\\n        dims=(\\\"bands\\\", \\\"y\\\", \\\"x\\\"),\\n        coords={\\n            \\\"bands\\\": [\\\"lon\\\", \\\"lat\\\"],\\n            \\\"x\\\": cube.coords[\\\"x\\\"],\\n            \\\"y\\\": cube.coords[\\\"y\\\"]\\n        }\\n    )\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"applydimension1\": {\"arguments\": {\"data\": {\"from_node\": \"mergecubes4\"}, \"dimension\": \"t\", \"process\": {\"process_graph\": {\"runudf2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"import numpy as np\\nimport xarray as xr\\nimport logging\\nfrom typing import Tuple, Dict\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(message)s'  # Show only the message\\n)\\nlogger = logging.getLogger(__name__)\\n\\n# Constants\\nNORM_PERCENTILES = np.array([\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7261204997060209, 2.038905204308012],\\n    [1.6798346251414997, 2.179592821212937],\\n    [2.3828939530384052, 2.7578332604178284],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351]\\n], dtype=np.float32)\\n\\nEXPECTED_BANDS = [\\n    \\\"B02\\\", \\\"B03\\\", \\\"B04\\\", \\\"B05\\\", \\\"B06\\\", \\\"B07\\\", \\\"B08\\\", \\\"B11\\\", \\\"B12\\\",\\n    \\\"NDVI\\\", \\\"VV\\\", \\\"VH\\\", \\\"DEM\\\", \\\"lon\\\", \\\"lat\\\"\\n]\\n\\n# --- Normalization helpers ---\\n\\ndef normalise_vv(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 25) / 25, 0, 1).astype(np.float32)\\n\\ndef normalise_vh(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 30) / 25, 0, 1).astype(np.float32)\\n\\ndef normalise_longitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 180) / 360, 0, 1).astype(np.float32)\\n\\ndef normalise_latitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 60) / 120, 0, 1).astype(np.float32)\\n\\ndef normalise_altitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 400) / 8400, 0, 1).astype(np.float32)\\n\\ndef normalise_ndvi(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 1) / 2, 0, 1).astype(np.float32)\\n\\ndef norm_optical(image: np.ndarray) -> np.ndarray:\\n    min_values = NORM_PERCENTILES[:, 0].reshape(9, 1, 1)\\n    scale_values = NORM_PERCENTILES[:, 1].reshape(9, 1, 1)\\n\\n    image = np.log(image * 0.005 + 1)\\n    image = (image - min_values) / scale_values\\n    image = np.exp(image * 5 - 1)\\n    return (image / (image + 1)).astype(np.float32)\\n\\n# --- Core logic ---\\n\\ndef validate_and_reorder_bands(\\n    data: xr.DataArray\\n) -> Tuple[xr.DataArray, Dict[str, int]]:\\n    \\\"\\\"\\\"\\n    Ensure the input DataArray has all EXPECTED_BANDS in the correct order.\\n\\n    Returns:\\n        Tuple of (reordered data, band name to index mapping).\\n        Raises ValueError if bands are missing.\\n    \\\"\\\"\\\"\\n    current = list(data.coords[\\\"bands\\\"].values)\\n\\n    if current != EXPECTED_BANDS:\\n        try:\\n            data = data.sel(bands=EXPECTED_BANDS)\\n            logger.info(f\\\"Reordered bands from {current} to {EXPECTED_BANDS}\\\")\\n        except KeyError:\\n            missing = sorted(set(EXPECTED_BANDS) - set(current))\\n            logger.error(f\\\"Missing required band(s): {missing}. Available: {current}\\\")\\n            raise ValueError(f\\\"Missing required band(s): {missing}\\\")\\n\\n    band_names = list(data.coords[\\\"bands\\\"].values)\\n    try:\\n        band_indices = {b: band_names.index(b) for b in EXPECTED_BANDS}\\n        logger.info(f\\\"Band indices mapped: {band_indices}\\\")\\n    except ValueError:\\n        logger.error(f\\\"Failed to map indices. Bands present: {band_names}\\\")\\n        raise ValueError(f\\\"Band order mismatch. Expected: {EXPECTED_BANDS}, Got: {band_names}\\\")\\n\\n    return data, band_indices\\n\\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Normalize input datacube for inference:\\n    - Optical bands (first 9): via log-transform then sigmoid-like scale\\n    - Others (VV, VH, NDVI, DEM, lon, lat): via fixed min/max scalings\\n\\n    Args:\\n        cube (xr.DataArray): Input cube with dims (bands, y, x, t)\\n        context (dict): Unused, placeholder for UDF interface\\n\\n    Returns:\\n        Normalized xr.DataArray with shape (15, y, x, t)\\n    \\\"\\\"\\\"\\n    cube = cube.transpose(\\\"bands\\\", \\\"y\\\", \\\"x\\\", \\\"t\\\")\\n    logger.info(f\\\"Received data with shape: {cube.shape} and dims: {cube.dims}\\\")\\n\\n    reordered, band_idx = validate_and_reorder_bands(cube)\\n    vals = reordered.values\\n\\n    # 1) Optical bands normalization\\n    optical = vals[:9, ...]\\n    mins = NORM_PERCENTILES[:, 0].reshape(9, 1, 1, 1)\\n    scales = NORM_PERCENTILES[:, 1].reshape(9, 1, 1, 1)\\n\\n    normed_opt = np.log(optical * 0.005 + 1)\\n    normed_opt = (normed_opt - mins) / scales\\n    normed_opt = np.exp(normed_opt * 5 - 1)\\n    normed_opt = (normed_opt / (normed_opt + 1)).astype(np.float32)\\n\\n    # 2) Scalar band normalization\\n    ndvi = normalise_ndvi(vals[band_idx[\\\"NDVI\\\"], ...])\\n    vv   = normalise_vv(vals[band_idx[\\\"VV\\\"], ...])\\n    vh   = normalise_vh(vals[band_idx[\\\"VH\\\"], ...])\\n    dem  = normalise_altitude(vals[band_idx[\\\"DEM\\\"], ...])\\n    lon  = normalise_longitude(vals[band_idx[\\\"lon\\\"], ...])\\n    lat  = normalise_latitude(vals[band_idx[\\\"lat\\\"], ...])\\n\\n    # 3) Concatenate all normalized bands\\n    output = np.concatenate(\\n        [normed_opt, ndvi[None], vv[None], vh[None], dem[None], lon[None], lat[None]],\\n        axis=0\\n    )\\n\\n    return xr.DataArray(\\n        output,\\n        dims=(\\\"bands\\\", \\\"y\\\", \\\"x\\\", \\\"t\\\"),\\n        coords={**reordered.coords, \\\"bands\\\": EXPECTED_BANDS}\\n    )\\n\\n\\n    \\n\\n    \\n\\n\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applyneighborhood1\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension1\"}, \"overlap\": [{\"dimension\": \"x\", \"unit\": \"px\", \"value\": 0}, {\"dimension\": \"y\", \"unit\": \"px\", \"value\": 0}], \"process\": {\"process_graph\": {\"runudf3\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"import sys\\nimport functools\\nimport numpy as np\\nimport xarray as xr\\nimport logging\\nfrom typing import Dict, Tuple\\n\\n\\n\\n# Setup logger\\ndef _setup_logging():\\n    logging.basicConfig(level=logging.INFO)\\n    return logging.getLogger(__name__)\\n\\nlogger = _setup_logging()\\n\\n# Add ONNX paths\\nsys.path.append(\\\"onnx_deps\\\")\\nsys.path.append(\\\"onnx_models\\\")\\nimport onnxruntime as ort\\n\\n# Constants for sanitization\\n_INF_REPLACEMENT = 1e6\\n_NEG_INF_REPLACEMENT = -1e6\\n\\n@functools.lru_cache(maxsize=1)\\ndef _load_ort_session(model_name: str) -> ort.InferenceSession:\\n    \\\"\\\"\\\"Loads an ONNX model and returns a cached ONNX runtime session.\\\"\\\"\\\"\\n    return ort.InferenceSession(f\\\"onnx_models/{model_name}\\\")\\n\\n@functools.lru_cache(maxsize=1)\\ndef build_gaussian_mask(height: int, width: int, sigma: float = 0.5) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    Builds a 2D Gaussian mask for CNN-style patch blending.\\n    \\n    sigma: relative spread (e.g., 0.125 \\u2192 ~1/8th of patch size)\\n    \\\"\\\"\\\"\\n    y = np.linspace(-1, 1, height)\\n    x = np.linspace(-1, 1, width)\\n    xx, yy = np.meshgrid(x, y)\\n    gaussian = np.exp(-0.5 * ((xx**2 + yy**2) / sigma**2))\\n    return gaussian.astype(np.float32)\\n\\n\\n\\ndef preprocess_image(cube: xr.DataArray) -> Tuple[np.ndarray, Dict[str, xr.Coordinate], np.ndarray]:\\n    \\\"\\\"\\\"\\n    Prepare the input cube for inference:\\n      - Transpose to (y, x, bands)\\n      - Sanitize NaN/Inf\\n      - Return batch tensor, coords, and invalid-value mask\\n    \\\"\\\"\\\"\\n    # Reorder dims\\n    reordered = cube.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    values = reordered.values.astype(np.float32)\\n\\n    # Mask invalid entries\\n    mask_invalid = ~np.isfinite(values)\\n\\n    # Replace NaN with 0, inf with large sentinel\\n    sanitized = np.where(np.isnan(values), 0.0, values)\\n    sanitized = np.where(np.isposinf(sanitized), _INF_REPLACEMENT, sanitized)\\n    sanitized = np.where(np.isneginf(sanitized), _NEG_INF_REPLACEMENT, sanitized)\\n\\n    # Add batch dimension\\n    input_tensor = sanitized[None, ...]\\n    logger.info(f\\\"Preprocessed tensor shape={input_tensor.shape}\\\")\\n    return input_tensor, reordered.coords, mask_invalid\\n\\n\\ndef run_inference(\\n    session: ort.InferenceSession,\\n    input_name: str,\\n    input_tensor: np.ndarray\\n) -> np.ndarray:\\n    \\\"\\\"\\\"Run ONNX session and remove batch dimension from output.\\\"\\\"\\\"\\n    outputs = session.run(None, {input_name: input_tensor})\\n    pred = np.squeeze(outputs[0], axis=0)\\n    logger.info(f\\\"Inference output shape={pred.shape}\\\")\\n    return pred\\n\\n\\ndef postprocess_output(\\n    pred: np.ndarray,\\n    coords: Dict[str, xr.Coordinate],\\n    mask_invalid: np.ndarray\\n) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Combine class predictions and probabilities into DataArray,\\n    restoring NaNs for originally invalid pixels.\\n    \\\"\\\"\\\"\\n    # Remove background class (class 0)\\n    scores = pred[..., 1:].astype(np.float32)\\n\\n    # Normalize probabilities across class axis\\n    score_sums = np.sum(scores, axis=-1, keepdims=True)\\n    normalized_scores = np.divide(\\n        scores,\\n        score_sums,\\n        out=np.zeros_like(scores),\\n        where=score_sums != 0\\n    )\\n\\n    normalized_scores *= 100.0\\n\\n    gausian_mask = build_gaussian_mask(normalized_scores.shape[0], normalized_scores.shape[1])\\n    # Apply to each class score (broadcast along last dimension)\\n    weighted_scores = normalized_scores * gausian_mask[..., None]\\n\\n    # Restore invalid pixels as NaN\\n    invalid_any = np.any(mask_invalid, axis=-1)\\n    weighted_scores[invalid_any] = 101 #TODO which value to set?\\n\\n    output_arr = np.concatenate([gausian_mask[..., None], weighted_scores], axis=-1)\\n\\n    # Build DataArray\\n    y_coords = coords[\\\"y\\\"]\\n    x_coords = coords[\\\"x\\\"]\\n    band_coords = np.arange(output_arr.shape[-1])\\n\\n    return xr.DataArray(\\n        output_arr,\\n        dims=(\\\"y\\\", \\\"x\\\", \\\"bands\\\"),\\n        coords={\\\"y\\\": y_coords, \\\"x\\\": x_coords, \\\"bands\\\": band_coords}\\n    )\\n\\n\\ndef apply_model(\\n    cube: xr.DataArray,\\n    model_path: str\\n) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Full inference pipeline: preprocess, infer, postprocess.\\n    \\\"\\\"\\\"\\n    input_tensor, coords, mask_invalid = preprocess_image(cube)\\n    session = _load_ort_session(model_path)\\n    input_name = session.get_inputs()[0].name\\n    raw_pred = run_inference(session, input_name, input_tensor)\\n    \\n    result = postprocess_output(raw_pred, coords, mask_invalid)\\n    logger.info(f\\\"apply_model result shape={result.shape}\\\")\\n    return result\\n\\ndef _pad_to_64(arr: np.ndarray, pad_value: float = np.nan) -> Tuple[np.ndarray, Tuple[int, int], Tuple[int, int]]:\\n    \\\"\\\"\\\"\\n    Given an array `arr` of shape (y_sub, x_sub, bands), where y_sub<=64 and x_sub<=64,\\n    pad it to exactly (64, 64, bands) by adding `pad_value` on bottom/right.\\n    Returns:\\n      - padded array of shape (64,64,bands)\\n      - (y_pad_before, y_pad_after)  \\u2192 e.g. (0, 64-y_sub)\\n      - (x_pad_before, x_pad_after)  \\u2192 e.g. (0, 64-x_sub)\\n    \\\"\\\"\\\"\\n    y_sub, x_sub, bands = arr.shape\\n    pad_y = 64 - y_sub\\n    pad_x = 64 - x_sub\\n    pad_width = [(0, pad_y), (0, pad_x), (0, 0)]  # pad in bottom/right only\\n    padded = np.pad(arr, pad_width, mode=\\\"constant\\\", constant_values=pad_value)\\n    return padded, (0, pad_y), (0, pad_x)\\n\\ndef _unpad_from_64(\\n    result_patch: np.ndarray,\\n    y_pad: Tuple[int, int],\\n    x_pad: Tuple[int, int]\\n) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    Given `result_patch` of shape (64,64,bands_out) and the paddings\\n    y_pad=(y_before,y_after), x_pad=(x_before,x_after), slice away the padded\\n    rows/columns and return an array of shape (64 - (y_before+y_after),\\n    64 - (x_before+x_after), bands_out).\\n    \\\"\\\"\\\"\\n    y_before, y_after = y_pad\\n    x_before, x_after = x_pad\\n    y_stop = 64 - y_after\\n    x_stop = 64 - x_after\\n    return result_patch[y_before:y_stop, x_before:x_stop, :]\\n\\n\\ndef apply_model_on_tiles(\\n    cube: xr.DataArray,\\n    model_path: str,\\n    tile_size: int = 64\\n) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Given a single\\u2010timestep cube of shape (y_full, x_full, bands),\\n    break it into non\\u2010overlapping tile_size\\u00d7tile_size patches (padding edges),\\n    call apply_model() on each 64\\u00d764 patch, then unpad & stitch back.\\n\\n    Returns: one DataArray of shape (y_full, x_full, bands_out).\\n    \\\"\\\"\\\"\\n    y_full = cube.sizes[\\\"y\\\"]\\n    x_full = cube.sizes[\\\"x\\\"]\\n\\n    # How many tiles along y and x?\\n    ny = int(np.ceil(y_full / tile_size))\\n    nx = int(np.ceil(x_full / tile_size))\\n\\n    # Slice y=[0:64], x=[0:64] (clamped if smaller)\\n    top = cube.isel(\\n        y=slice(0, min(tile_size, y_full)),\\n        x=slice(0, min(tile_size, x_full))\\n    )\\n    arr0 = top.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\").values.astype(np.float32)\\n    padded0, y_pad0, x_pad0 = _pad_to_64(arr0, pad_value=np.nan)\\n\\n    # Wrap into a DataArray so apply_model can run\\n    da0 = xr.DataArray(\\n        padded0,\\n        dims=(\\\"y\\\", \\\"x\\\", \\\"bands\\\"),\\n        coords={\\n            \\\"y\\\": np.arange(tile_size),\\n            \\\"x\\\": np.arange(tile_size),\\n            \\\"bands\\\": top.coords[\\\"bands\\\"]\\n        }\\n    )\\n    out0 = apply_model(da0, model_path)\\n    bands_out = out0.sizes[\\\"bands\\\"]\\n\\n    # Prepare a big container for the stitched result\\n    stitched = np.full((y_full, x_full, bands_out), np.nan, dtype=np.float32)\\n\\n\\n    for iy in range(ny):\\n        for ix in range(nx):\\n            y0 = iy * tile_size\\n            x0 = ix * tile_size\\n            y1 = min(y0 + tile_size, y_full)\\n            x1 = min(x0 + tile_size, x_full)\\n\\n            # Extract that sub\\u2010window\\n            sub = cube.isel(\\n                y=slice(y0, y1),\\n                x=slice(x0, x1)\\n            )\\n            arr_sub = sub.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\").values.astype(np.float32)\\n\\n            # Pad if needed (only on bottom/right)\\n            padded, y_pad, x_pad = _pad_to_64(arr_sub, pad_value=np.nan)\\n\\n            # Wrap padded into DataArray for apply_model\\n            da_pad = xr.DataArray(\\n                padded,\\n                dims=(\\\"y\\\", \\\"x\\\", \\\"bands\\\"),\\n                coords={\\n                    \\\"y\\\": np.arange(tile_size),\\n                    \\\"x\\\": np.arange(tile_size),\\n                    \\\"bands\\\": sub.coords[\\\"bands\\\"]\\n                }\\n            )\\n\\n            # Run the full preprocess \\u2192 inference \\u2192 postprocess for this tile\\n            out_patch = apply_model(da_pad, model_path)  \\n            # out_patch is (64,64,bands_out)\\n\\n            # Unpad: remove any rows/cols that we added\\n            arr_out_patch = out_patch.values  # (64,64,bands_out)\\n            unpadded = _unpad_from_64(arr_out_patch, y_pad, x_pad)\\n            # Now unpadded.shape == (y1-y0, x1-x0, bands_out)\\n\\n            # Write into the stitched array\\n            stitched[y0:y1, x0:x1, :] = unpadded\\n\\n    return xr.DataArray(\\n        stitched,\\n        dims=(\\\"y\\\", \\\"x\\\", \\\"bands\\\"),\\n        coords={\\n            \\\"y\\\": cube.coords[\\\"y\\\"].values,\\n            \\\"x\\\": cube.coords[\\\"x\\\"].values,\\n            \\\"bands\\\": np.arange(bands_out)\\n        }\\n    )\\n\\n\\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Apply ONNX model per timestep in the datacube.\\n    \\\"\\\"\\\"\\n    logger.info(f\\\"apply_datacube received shape={cube.shape}, dims={cube.dims}\\\")\\n    cube = cube.transpose('y', 'x', 'bands', 't')\\n\\n    if 't' in cube.dims:\\n        logger.info(\\\"Applying model per timestep via groupby-map.\\\")\\n        return cube.groupby('t').map(lambda da: apply_model_on_tiles(da, \\\"WAC_model_hansvrp.onnx\\\"))\\n    else:\\n        logger.info(\\\"Single timestep: applying model once.\\\")\\n        return apply_model_on_tiles(cube, \\\"WAC_model_hansvrp.onnx\\\")\"}, \"process_id\": \"run_udf\", \"result\": true}}}, \"size\": [{\"dimension\": \"x\", \"unit\": \"px\", \"value\": 64}, {\"dimension\": \"y\", \"unit\": \"px\", \"value\": 64}]}, \"process_id\": \"apply_neighborhood\"}, \"loadcollection1\": {\"arguments\": {\"bands\": [\"VV\", \"VH\"], \"id\": \"SENTINEL1_GLOBAL_MOSAICS\", \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305000, \"north\": 9805000, \"south\": 9800000, \"west\": 300000}, \"temporal_extent\": [\"2023-05-31\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection2\": {\"arguments\": {\"bands\": [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte1\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 85}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305000, \"north\": 9805000, \"south\": 9800000, \"west\": 300000}, \"temporal_extent\": [\"2023-06-01\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection3\": {\"arguments\": {\"bands\": [\"SCL\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte2\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 85}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305000, \"north\": 9805000, \"south\": 9800000, \"west\": 300000}, \"temporal_extent\": [\"2023-06-01\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection4\": {\"arguments\": {\"id\": \"COPERNICUS_30\", \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305000, \"north\": 9805000, \"south\": 9800000, \"west\": 300000}, \"temporal_extent\": null}, \"process_id\": \"load_collection\"}, \"mask1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial2\"}, \"mask\": {\"from_node\": \"toscldilationmask1\"}}, \"process_id\": \"mask\"}, \"mergecubes1\": {\"arguments\": {\"cube1\": {\"from_node\": \"apply1\"}, \"cube2\": {\"from_node\": \"aggregatetemporalperiod1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes2\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes1\"}, \"cube2\": {\"from_node\": \"adddimension1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes3\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes2\"}, \"cube2\": {\"from_node\": \"reducedimension1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes4\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes3\"}, \"cube2\": {\"from_node\": \"renamelabels1\"}}, \"process_id\": \"merge_cubes\"}, \"ndvi1\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod1\"}, \"nir\": \"B08\", \"red\": \"B04\"}, \"process_id\": \"ndvi\"}, \"reducedimension1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial4\"}, \"dimension\": \"t\", \"reducer\": {\"process_graph\": {\"mean2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\", \"result\": true}}}}, \"process_id\": \"reduce_dimension\"}, \"renamelabels1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial5\"}, \"dimension\": \"bands\", \"target\": [\"lon\", \"lat\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels2\": {\"arguments\": {\"data\": {\"from_node\": \"applyneighborhood1\"}, \"dimension\": \"bands\", \"target\": [\"mask_weights\", \"prob_class_0\", \"prob_class_1\", \"prob_class_2\", \"prob_class_3\", \"prob_class_4\", \"prob_class_5\", \"prob_class_6\", \"prob_class_7\", \"prob_class_8\", \"prob_class_9\", \"prob_class_10\", \"prob_class_11\", \"prob_class_12\", \"prob_class_13\", \"prob_class_14\", \"prob_class_15\", \"prob_class_16\", \"prob_class_17\", \"prob_class_18\", \"prob_class_19\", \"prob_class_20\"]}, \"process_id\": \"rename_labels\"}, \"resamplespatial1\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection1\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial2\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection2\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial3\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection3\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial4\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection4\"}, \"method\": \"bilinear\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial5\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"apply2\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"saveresult1\": {\"arguments\": {\"data\": {\"from_node\": \"renamelabels2\"}, \"format\": \"netCDF\", \"options\": {}}, \"process_id\": \"save_result\", \"result\": true}, \"toscldilationmask1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial3\"}}, \"process_id\": \"to_scl_dilation_mask\"}}}, \"progress\": 100, \"status\": \"finished\", \"updated\": \"2025-07-18T11:50:12Z\", \"usage\": {\"cpu\": {\"unit\": \"cpu-seconds\", \"value\": 1550.141373034}, \"duration\": {\"unit\": \"seconds\", \"value\": 386}, \"input_pixel\": {\"unit\": \"mega-pixel\", \"value\": 48.49079895019531}, \"max_executor_memory\": {\"unit\": \"gb\", \"value\": 2.5749893188476562}, \"memory\": {\"unit\": \"mb-seconds\", \"value\": 18629480.218526784}, \"network_received\": {\"unit\": \"b\", \"value\": 10807136967}}}}</script>\n",
       "    </openeo-job>\n",
       "    "
      ],
      "text/plain": [
       "<BatchJob job_id='j-2507181142564c2c914eccd469bc2c06'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "connection = openeo.connect(\"https://openeo.dataspace.copernicus.eu/\")\n",
    "connection.authenticate_oidc()\n",
    "\n",
    "new_band_names = ['mask_weights'] + [f\"prob_class_{c}\" for c in range(N_CLASSES)]\n",
    "ROI_cube = load_input_WAC(connection, SPATIAL_EXTENT, TEMPORAL_EXTENT, MAX_CLOUD_COVER, RESOLUTION, CRS)\n",
    "ROI_inference_cube = inference_WAC(ROI_cube, patch_size=PATCH_SIZE, overlap = OVERLAP_SIZE)\n",
    "#ROI_inference_cube = ROI_inference_cube.linear_scale_range(0, 101, 0, 101)\n",
    "ROI_inference_cube =ROI_inference_cube.rename_labels(dimension = 'bands', target = new_band_names)\n",
    "\n",
    "\n",
    "ratio_cube = ROI_inference_cube.save_result(format = \"netCDF\")\n",
    "\n",
    "\n",
    "job = connection.create_job(ratio_cube,\n",
    "    additional=JOB_OPTIONS\n",
    ")\n",
    "\n",
    "job.start_and_wait()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c90e75e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Git_projects/WAC/production/prediction/sliding_window_normalizedj-2506031012114f9f83d1bbd803b0d53a/openEO.nc'),\n",
       " WindowsPath('C:/Git_projects/WAC/production/prediction/sliding_window_normalizedj-2506031012114f9f83d1bbd803b0d53a/job-results.json')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"C:/Git_projects/WAC/production/prediction/sliding_window_normalized\" + job.job_id\n",
    "OUTPUT_FILE_ROI = f\"{OUTPUT_DIR}\"\n",
    "job.get_results().download_files(OUTPUT_FILE_ROI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8558bd5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00 Job 'j-250603080142426893ff124a7609890a': send 'start'\n",
      "0:00:13 Job 'j-250603080142426893ff124a7609890a': created (progress 0%)\n",
      "0:00:18 Job 'j-250603080142426893ff124a7609890a': created (progress 0%)\n",
      "0:00:25 Job 'j-250603080142426893ff124a7609890a': created (progress 0%)\n",
      "0:00:33 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:00:43 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:00:55 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:01:11 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:01:30 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:01:54 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:02:24 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:03:01 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:03:48 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:04:47 Job 'j-250603080142426893ff124a7609890a': running (progress N/A)\n",
      "0:05:47 Job 'j-250603080142426893ff124a7609890a': finished (progress 100%)\n",
      "0:00:00 Job 'j-2506030807304b9f9c7c0a45c080cc10': send 'start'\n",
      "0:00:12 Job 'j-2506030807304b9f9c7c0a45c080cc10': created (progress 0%)\n",
      "0:00:17 Job 'j-2506030807304b9f9c7c0a45c080cc10': created (progress 0%)\n",
      "0:00:24 Job 'j-2506030807304b9f9c7c0a45c080cc10': created (progress 0%)\n",
      "0:00:32 Job 'j-2506030807304b9f9c7c0a45c080cc10': created (progress 0%)\n",
      "0:00:42 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:00:54 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:01:09 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:01:29 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:01:53 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:02:23 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:03:00 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:03:47 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:04:45 Job 'j-2506030807304b9f9c7c0a45c080cc10': running (progress N/A)\n",
      "0:05:46 Job 'j-2506030807304b9f9c7c0a45c080cc10': finished (progress 100%)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <script>\n",
       "    if (!window.customElements || !window.customElements.get('openeo-job')) {\n",
       "        var el = document.createElement('script');\n",
       "        el.src = \"https://cdn.jsdelivr.net/npm/@openeo/vue-components@2/assets/openeo.min.js\";\n",
       "        document.head.appendChild(el);\n",
       "\n",
       "        var font = document.createElement('font');\n",
       "        font.as = \"font\";\n",
       "        font.type = \"font/woff2\";\n",
       "        font.crossOrigin = true;\n",
       "        font.href = \"https://use.fontawesome.com/releases/v5.13.0/webfonts/fa-solid-900.woff2\"\n",
       "        document.head.appendChild(font);\n",
       "    }\n",
       "    </script>\n",
       "    <openeo-job>\n",
       "        <script type=\"application/json\">{\"currency\": \"credits\", \"job\": {\"costs\": 8, \"created\": \"2025-06-03T08:07:30Z\", \"id\": \"j-2506030807304b9f9c7c0a45c080cc10\", \"process\": {\"process_graph\": {\"adddimension1\": {\"arguments\": {\"data\": {\"from_node\": \"ndvi1\"}, \"label\": \"NDVI\", \"name\": \"bands\", \"type\": \"bands\"}, \"process_id\": \"add_dimension\"}, \"aggregatetemporalperiod1\": {\"arguments\": {\"data\": {\"from_node\": \"mask1\"}, \"period\": \"month\", \"reducer\": {\"process_graph\": {\"mean1\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\", \"result\": true}}}}, \"process_id\": \"aggregate_temporal_period\"}, \"apply1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial1\"}, \"process\": {\"process_graph\": {\"log1\": {\"arguments\": {\"base\": 10, \"x\": {\"from_parameter\": \"x\"}}, \"process_id\": \"log\"}, \"multiply1\": {\"arguments\": {\"x\": 10, \"y\": {\"from_node\": \"log1\"}}, \"process_id\": \"multiply\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"apply2\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod1\"}, \"process\": {\"process_graph\": {\"runudf1\": {\"arguments\": {\"context\": {\"crs\": \"EPSG:3035\", \"east\": 305160, \"north\": 9805160, \"south\": 9799840, \"west\": 299840}, \"data\": {\"from_parameter\": \"x\"}, \"runtime\": \"Python\", \"udf\": \"import numpy as np\\nimport xarray as xr\\nimport logging\\nfrom pyproj import Transformer\\nfrom typing import Dict\\n\\n# Setup logging\\ndef _setup_logging() -> logging.Logger:\\n    logging.basicConfig(level=logging.INFO, format=\\\"%(message)s\\\")\\n    return logging.getLogger(__name__)\\n\\nlogger = _setup_logging()\\n\\ndef apply_datacube(cube: xr.DataArray, context: Dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Constructs a lon/lat grid as a new DataArray based on the cube's spatial resolution\\n    and the geographic extent provided in `context`.\\n\\n    Args:\\n        cube (xr.DataArray): Input data cube with 'x' and 'y' dimensions.\\n        context (dict): Dictionary containing 'west', 'south', 'east', 'north', and 'crs'.\\n\\n    Returns:\\n        xr.DataArray: A new DataArray of shape (2, y, x) with bands ['lon', 'lat'].\\n    \\\"\\\"\\\"\\n\\n    # Parse extent and CRS\\n    try:\\n        west  = float(context[\\\"west\\\"])\\n        south = float(context[\\\"south\\\"])\\n        east  = float(context[\\\"east\\\"])\\n        north = float(context[\\\"north\\\"])\\n        crs   = context[\\\"crs\\\"]\\n    except KeyError as e:\\n        raise ValueError(f\\\"Missing required context key: {e}\\\")\\n\\n    logger.info(f\\\"Original extent: {west}, {south} \\u2192 {east}, {north} in {crs}\\\")\\n\\n    # Transform extent to EPSG:4326 if needed\\n    if crs != \\\"EPSG:4326\\\":\\n        transformer = Transformer.from_crs(crs, \\\"EPSG:4326\\\", always_xy=True)\\n        west, south = transformer.transform(west, south)\\n        east, north = transformer.transform(east, north)\\n        logger.info(f\\\"Transformed extent to EPSG:4326: {west}, {south} \\u2192 {east}, {north}\\\")\\n\\n    # Get cube dimensions\\n    nx = cube.sizes[\\\"x\\\"]\\n    ny = cube.sizes[\\\"y\\\"]\\n\\n    # Create lon/lat coordinate arrays\\n    lon = np.linspace(west, east, nx, dtype=np.float32)\\n    lat = np.linspace(north, south, ny, dtype=np.float32)  # north \\u2192 south to match image orientation\\n\\n    # Generate 2D meshgrid\\n    lon_grid, lat_grid = np.meshgrid(lon, lat)\\n\\n    logger.info(f\\\"Longitude range: {lon_grid.min()} to {lon_grid.max()}\\\")\\n    logger.info(f\\\"Latitude range: {lat_grid.min()} to {lat_grid.max()}\\\")\\n\\n    # Build output DataArray\\n    return xr.DataArray(\\n        data=np.stack([lon_grid, lat_grid], axis=0),  # shape: (2, y, x)\\n        dims=(\\\"bands\\\", \\\"y\\\", \\\"x\\\"),\\n        coords={\\n            \\\"bands\\\": [\\\"lon\\\", \\\"lat\\\"],\\n            \\\"x\\\": cube.coords[\\\"x\\\"],\\n            \\\"y\\\": cube.coords[\\\"y\\\"]\\n        }\\n    )\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply\"}, \"applydimension1\": {\"arguments\": {\"data\": {\"from_node\": \"mergecubes4\"}, \"dimension\": \"t\", \"process\": {\"process_graph\": {\"runudf2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"import numpy as np\\nimport xarray as xr\\nimport logging\\nfrom typing import Tuple, Dict\\n\\n# Configure logging\\nlogging.basicConfig(\\n    level=logging.INFO,\\n    format='%(message)s'  # Show only the message\\n)\\nlogger = logging.getLogger(__name__)\\n\\n# Constants\\nNORM_PERCENTILES = np.array([\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7261204997060209, 2.038905204308012],\\n    [1.6798346251414997, 2.179592821212937],\\n    [2.3828939530384052, 2.7578332604178284],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351],\\n    [1.7417268007636313, 2.023298706048351]\\n], dtype=np.float32)\\n\\nEXPECTED_BANDS = [\\n    \\\"B02\\\", \\\"B03\\\", \\\"B04\\\", \\\"B05\\\", \\\"B06\\\", \\\"B07\\\", \\\"B08\\\", \\\"B11\\\", \\\"B12\\\",\\n    \\\"NDVI\\\", \\\"VV\\\", \\\"VH\\\", \\\"DEM\\\", \\\"lon\\\", \\\"lat\\\"\\n]\\n\\n# --- Normalization helpers ---\\n\\ndef normalise_vv(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 25) / 25, 0, 1).astype(np.float32)\\n\\ndef normalise_vh(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 30) / 25, 0, 1).astype(np.float32)\\n\\ndef normalise_longitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 180) / 360, 0, 1).astype(np.float32)\\n\\ndef normalise_latitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 60) / 120, 0, 1).astype(np.float32)\\n\\ndef normalise_altitude(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 400) / 8400, 0, 1).astype(np.float32)\\n\\ndef normalise_ndvi(raster: np.ndarray) -> np.ndarray:\\n    return np.clip((raster + 1) / 2, 0, 1).astype(np.float32)\\n\\ndef norm_optical(image: np.ndarray) -> np.ndarray:\\n    min_values = NORM_PERCENTILES[:, 0].reshape(9, 1, 1)\\n    scale_values = NORM_PERCENTILES[:, 1].reshape(9, 1, 1)\\n\\n    image = np.log(image * 0.005 + 1)\\n    image = (image - min_values) / scale_values\\n    image = np.exp(image * 5 - 1)\\n    return (image / (image + 1)).astype(np.float32)\\n\\n# --- Core logic ---\\n\\ndef validate_and_reorder_bands(\\n    data: xr.DataArray\\n) -> Tuple[xr.DataArray, Dict[str, int]]:\\n    \\\"\\\"\\\"\\n    Ensure the input DataArray has all EXPECTED_BANDS in the correct order.\\n\\n    Returns:\\n        Tuple of (reordered data, band name to index mapping).\\n        Raises ValueError if bands are missing.\\n    \\\"\\\"\\\"\\n    current = list(data.coords[\\\"bands\\\"].values)\\n\\n    if current != EXPECTED_BANDS:\\n        try:\\n            data = data.sel(bands=EXPECTED_BANDS)\\n            logger.info(f\\\"Reordered bands from {current} to {EXPECTED_BANDS}\\\")\\n        except KeyError:\\n            missing = sorted(set(EXPECTED_BANDS) - set(current))\\n            logger.error(f\\\"Missing required band(s): {missing}. Available: {current}\\\")\\n            raise ValueError(f\\\"Missing required band(s): {missing}\\\")\\n\\n    band_names = list(data.coords[\\\"bands\\\"].values)\\n    try:\\n        band_indices = {b: band_names.index(b) for b in EXPECTED_BANDS}\\n        logger.info(f\\\"Band indices mapped: {band_indices}\\\")\\n    except ValueError:\\n        logger.error(f\\\"Failed to map indices. Bands present: {band_names}\\\")\\n        raise ValueError(f\\\"Band order mismatch. Expected: {EXPECTED_BANDS}, Got: {band_names}\\\")\\n\\n    return data, band_indices\\n\\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Normalize input datacube for inference:\\n    - Optical bands (first 9): via log-transform then sigmoid-like scale\\n    - Others (VV, VH, NDVI, DEM, lon, lat): via fixed min/max scalings\\n\\n    Args:\\n        cube (xr.DataArray): Input cube with dims (bands, y, x, t)\\n        context (dict): Unused, placeholder for UDF interface\\n\\n    Returns:\\n        Normalized xr.DataArray with shape (15, y, x, t)\\n    \\\"\\\"\\\"\\n    cube = cube.transpose(\\\"bands\\\", \\\"y\\\", \\\"x\\\", \\\"t\\\")\\n    logger.info(f\\\"Received data with shape: {cube.shape} and dims: {cube.dims}\\\")\\n\\n    reordered, band_idx = validate_and_reorder_bands(cube)\\n    vals = reordered.values\\n\\n    # 1) Optical bands normalization\\n    optical = vals[:9, ...]\\n    mins = NORM_PERCENTILES[:, 0].reshape(9, 1, 1, 1)\\n    scales = NORM_PERCENTILES[:, 1].reshape(9, 1, 1, 1)\\n\\n    normed_opt = np.log(optical * 0.005 + 1)\\n    normed_opt = (normed_opt - mins) / scales\\n    normed_opt = np.exp(normed_opt * 5 - 1)\\n    normed_opt = (normed_opt / (normed_opt + 1)).astype(np.float32)\\n\\n    # 2) Scalar band normalization\\n    ndvi = normalise_ndvi(vals[band_idx[\\\"NDVI\\\"], ...])\\n    vv   = normalise_vv(vals[band_idx[\\\"VV\\\"], ...])\\n    vh   = normalise_vh(vals[band_idx[\\\"VH\\\"], ...])\\n    dem  = normalise_altitude(vals[band_idx[\\\"DEM\\\"], ...])\\n    lon  = normalise_longitude(vals[band_idx[\\\"lon\\\"], ...])\\n    lat  = normalise_latitude(vals[band_idx[\\\"lat\\\"], ...])\\n\\n    # 3) Concatenate all normalized bands\\n    output = np.concatenate(\\n        [normed_opt, ndvi[None], vv[None], vh[None], dem[None], lon[None], lat[None]],\\n        axis=0\\n    )\\n\\n    return xr.DataArray(\\n        output,\\n        dims=(\\\"bands\\\", \\\"y\\\", \\\"x\\\", \\\"t\\\"),\\n        coords={**reordered.coords, \\\"bands\\\": EXPECTED_BANDS}\\n    )\\n\\n\\n    \\n\\n    \\n\\n\"}, \"process_id\": \"run_udf\", \"result\": true}}}}, \"process_id\": \"apply_dimension\"}, \"applyneighborhood1\": {\"arguments\": {\"data\": {\"from_node\": \"applydimension1\"}, \"overlap\": [{\"dimension\": \"x\", \"unit\": \"px\", \"value\": 0}, {\"dimension\": \"y\", \"unit\": \"px\", \"value\": 0}], \"process\": {\"process_graph\": {\"runudf3\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}, \"runtime\": \"Python\", \"udf\": \"import sys\\nimport functools\\nimport numpy as np\\nimport xarray as xr\\nimport logging\\nfrom typing import Dict, Tuple\\n\\n\\n\\n# Setup logger\\ndef _setup_logging():\\n    logging.basicConfig(level=logging.INFO)\\n    return logging.getLogger(__name__)\\n\\nlogger = _setup_logging()\\n\\n# Add ONNX paths\\nsys.path.append(\\\"onnx_deps\\\")\\nsys.path.append(\\\"onnx_models\\\")\\nimport onnxruntime as ort\\n\\n# Constants for sanitization\\n_INF_REPLACEMENT = 1e6\\n_NEG_INF_REPLACEMENT = -1e6\\n\\n@functools.lru_cache(maxsize=1)\\ndef _load_ort_session(model_name: str) -> ort.InferenceSession:\\n    \\\"\\\"\\\"Loads an ONNX model and returns a cached ONNX runtime session.\\\"\\\"\\\"\\n    return ort.InferenceSession(f\\\"onnx_models/{model_name}\\\")\\n\\n@functools.lru_cache(maxsize=1)\\ndef build_gaussian_mask(height: int, width: int, sigma: float = 0.125) -> np.ndarray:\\n    \\\"\\\"\\\"\\n    Builds a 2D Gaussian mask for CNN-style patch blending.\\n    \\n    sigma: relative spread (e.g., 0.125 \\u2192 ~1/8th of patch size)\\n    \\\"\\\"\\\"\\n    y = np.linspace(-1, 1, height)\\n    x = np.linspace(-1, 1, width)\\n    xx, yy = np.meshgrid(x, y)\\n    gaussian = np.exp(-0.5 * ((xx**2 + yy**2) / sigma**2))\\n    return gaussian.astype(np.float32)\\n\\n\\ndef preprocess_image(cube: xr.DataArray) -> Tuple[np.ndarray, Dict[str, xr.Coordinate], np.ndarray]:\\n    \\\"\\\"\\\"\\n    Prepare the input cube for inference:\\n      - Transpose to (y, x, bands)\\n      - Sanitize NaN/Inf\\n      - Return batch tensor, coords, and invalid-value mask\\n    \\\"\\\"\\\"\\n    # Reorder dims\\n    reordered = cube.transpose(\\\"y\\\", \\\"x\\\", \\\"bands\\\")\\n    values = reordered.values.astype(np.float32)\\n\\n    # Mask invalid entries\\n    mask_invalid = ~np.isfinite(values)\\n\\n    # Replace NaN with 0, inf with large sentinel\\n    sanitized = np.where(np.isnan(values), 0.0, values)\\n    sanitized = np.where(np.isposinf(sanitized), _INF_REPLACEMENT, sanitized)\\n    sanitized = np.where(np.isneginf(sanitized), _NEG_INF_REPLACEMENT, sanitized)\\n\\n    # Add batch dimension\\n    input_tensor = sanitized[None, ...]\\n    logger.info(f\\\"Preprocessed tensor shape={input_tensor.shape}\\\")\\n    return input_tensor, reordered.coords, mask_invalid\\n\\n\\ndef run_inference(\\n    session: ort.InferenceSession,\\n    input_name: str,\\n    input_tensor: np.ndarray\\n) -> np.ndarray:\\n    \\\"\\\"\\\"Run ONNX session and remove batch dimension from output.\\\"\\\"\\\"\\n    outputs = session.run(None, {input_name: input_tensor})\\n    pred = np.squeeze(outputs[0], axis=0)\\n    logger.info(f\\\"Inference output shape={pred.shape}\\\")\\n    return pred\\n\\n\\ndef postprocess_output(\\n    pred: np.ndarray,\\n    coords: Dict[str, xr.Coordinate],\\n    mask_invalid: np.ndarray\\n) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Combine class predictions and probabilities into DataArray,\\n    restoring NaNs for originally invalid pixels.\\n    \\\"\\\"\\\"\\n    # Remove background class (class 0)\\n    scores = pred[..., 1:].astype(np.float32)\\n\\n    # Normalize probabilities across class axis\\n    score_sums = np.sum(scores, axis=-1, keepdims=True)\\n    normalized_scores = np.divide(\\n        scores,\\n        score_sums,\\n        out=np.zeros_like(scores),\\n        where=score_sums != 0\\n    )\\n\\n    normalized_scores *= 100.0\\n\\n    gausian_mask = build_gaussian_mask(normalized_scores.shape[0], normalized_scores.shape[1])\\n    # Apply to each class score (broadcast along last dimension)\\n    weighted_scores = normalized_scores * gausian_mask[..., None]\\n\\n    # Restore invalid pixels as NaN\\n    invalid_any = np.any(mask_invalid, axis=-1)\\n    weighted_scores[invalid_any] = -100 #TODO which value to set?\\n\\n    output_arr = np.concatenate([gausian_mask[..., None], weighted_scores], axis=-1)\\n\\n    # Build DataArray\\n    y_coords = coords[\\\"y\\\"]\\n    x_coords = coords[\\\"x\\\"]\\n    band_coords = np.arange(output_arr.shape[-1])\\n\\n    return xr.DataArray(\\n        output_arr,\\n        dims=(\\\"y\\\", \\\"x\\\", \\\"bands\\\"),\\n        coords={\\\"y\\\": y_coords, \\\"x\\\": x_coords, \\\"bands\\\": band_coords}\\n    )\\n\\n\\ndef apply_model(\\n    cube: xr.DataArray,\\n    model_path: str\\n) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Full inference pipeline: preprocess, infer, postprocess.\\n    \\\"\\\"\\\"\\n    input_tensor, coords, mask_invalid = preprocess_image(cube)\\n    session = _load_ort_session(model_path)\\n    input_name = session.get_inputs()[0].name\\n    raw_pred = run_inference(session, input_name, input_tensor)\\n    \\n    result = postprocess_output(raw_pred, coords, mask_invalid)\\n    logger.info(f\\\"apply_model result shape={result.shape}\\\")\\n    return result\\n\\n\\ndef apply_datacube(cube: xr.DataArray, context: dict) -> xr.DataArray:\\n    \\\"\\\"\\\"\\n    Apply ONNX model per timestep in the datacube.\\n    \\\"\\\"\\\"\\n    logger.info(f\\\"apply_datacube received shape={cube.shape}, dims={cube.dims}\\\")\\n    cube = cube.transpose('y', 'x', 'bands', 't')\\n\\n    if 't' in cube.dims:\\n        logger.info(\\\"Applying model per timestep via groupby-map.\\\")\\n        return cube.groupby('t').map(lambda da: apply_model(da, \\\"WAC_model_hansvrp.onnx\\\"))\\n    else:\\n        logger.info(\\\"Single timestep: applying model once.\\\")\\n        return apply_model(cube, \\\"WAC_model_hansvrp.onnx\\\")\"}, \"process_id\": \"run_udf\", \"result\": true}}}, \"size\": [{\"dimension\": \"x\", \"unit\": \"px\", \"value\": 64}, {\"dimension\": \"y\", \"unit\": \"px\", \"value\": 64}]}, \"process_id\": \"apply_neighborhood\"}, \"loadcollection1\": {\"arguments\": {\"bands\": [\"VV\", \"VH\"], \"id\": \"SENTINEL1_GLOBAL_MOSAICS\", \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305160, \"north\": 9805160, \"south\": 9799840, \"west\": 299840}, \"temporal_extent\": [\"2023-05-31\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection2\": {\"arguments\": {\"bands\": [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte1\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 85}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305160, \"north\": 9805160, \"south\": 9799840, \"west\": 299840}, \"temporal_extent\": [\"2023-06-01\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection3\": {\"arguments\": {\"bands\": [\"SCL\"], \"id\": \"SENTINEL2_L2A\", \"properties\": {\"eo:cloud_cover\": {\"process_graph\": {\"lte2\": {\"arguments\": {\"x\": {\"from_parameter\": \"value\"}, \"y\": 85}, \"process_id\": \"lte\", \"result\": true}}}}, \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305160, \"north\": 9805160, \"south\": 9799840, \"west\": 299840}, \"temporal_extent\": [\"2023-06-01\", \"2023-08-01\"]}, \"process_id\": \"load_collection\"}, \"loadcollection4\": {\"arguments\": {\"id\": \"COPERNICUS_30\", \"spatial_extent\": {\"crs\": \"EPSG:3035\", \"east\": 305160, \"north\": 9805160, \"south\": 9799840, \"west\": 299840}, \"temporal_extent\": null}, \"process_id\": \"load_collection\"}, \"mask1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial2\"}, \"mask\": {\"from_node\": \"toscldilationmask1\"}}, \"process_id\": \"mask\"}, \"mergecubes1\": {\"arguments\": {\"cube1\": {\"from_node\": \"apply1\"}, \"cube2\": {\"from_node\": \"aggregatetemporalperiod1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes2\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes1\"}, \"cube2\": {\"from_node\": \"adddimension1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes3\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes2\"}, \"cube2\": {\"from_node\": \"reducedimension1\"}}, \"process_id\": \"merge_cubes\"}, \"mergecubes4\": {\"arguments\": {\"cube1\": {\"from_node\": \"mergecubes3\"}, \"cube2\": {\"from_node\": \"renamelabels1\"}}, \"process_id\": \"merge_cubes\"}, \"ndvi1\": {\"arguments\": {\"data\": {\"from_node\": \"aggregatetemporalperiod1\"}, \"nir\": \"B08\", \"red\": \"B04\"}, \"process_id\": \"ndvi\"}, \"reducedimension1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial4\"}, \"dimension\": \"t\", \"reducer\": {\"process_graph\": {\"mean2\": {\"arguments\": {\"data\": {\"from_parameter\": \"data\"}}, \"process_id\": \"mean\", \"result\": true}}}}, \"process_id\": \"reduce_dimension\"}, \"renamelabels1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial5\"}, \"dimension\": \"bands\", \"target\": [\"lon\", \"lat\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels2\": {\"arguments\": {\"data\": {\"from_node\": \"applyneighborhood1\"}, \"dimension\": \"bands\", \"target\": [\"mask_weights\", \"prob_class_0\", \"prob_class_1\", \"prob_class_2\", \"prob_class_3\", \"prob_class_4\", \"prob_class_5\", \"prob_class_6\", \"prob_class_7\", \"prob_class_8\", \"prob_class_9\", \"prob_class_10\", \"prob_class_11\", \"prob_class_12\", \"prob_class_13\", \"prob_class_14\", \"prob_class_15\", \"prob_class_16\", \"prob_class_17\", \"prob_class_18\", \"prob_class_19\", \"prob_class_20\"]}, \"process_id\": \"rename_labels\"}, \"renamelabels3\": {\"arguments\": {\"data\": {\"from_node\": \"renamelabels2\"}, \"dimension\": \"bands\", \"target\": [\"mask_weights\", \"prob_class_0\", \"prob_class_1\", \"prob_class_2\", \"prob_class_3\", \"prob_class_4\", \"prob_class_5\", \"prob_class_6\", \"prob_class_7\", \"prob_class_8\", \"prob_class_9\", \"prob_class_10\", \"prob_class_11\", \"prob_class_12\", \"prob_class_13\", \"prob_class_14\", \"prob_class_15\", \"prob_class_16\", \"prob_class_17\", \"prob_class_18\", \"prob_class_19\", \"prob_class_20\"]}, \"process_id\": \"rename_labels\"}, \"resamplespatial1\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection1\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial2\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection2\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial3\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection3\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial4\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"loadcollection4\"}, \"method\": \"bilinear\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"resamplespatial5\": {\"arguments\": {\"align\": \"upper-left\", \"data\": {\"from_node\": \"apply2\"}, \"method\": \"near\", \"projection\": \"EPSG:3035\", \"resolution\": 10}, \"process_id\": \"resample_spatial\"}, \"saveresult1\": {\"arguments\": {\"data\": {\"from_node\": \"renamelabels3\"}, \"format\": \"netCDF\", \"options\": {}}, \"process_id\": \"save_result\", \"result\": true}, \"toscldilationmask1\": {\"arguments\": {\"data\": {\"from_node\": \"resamplespatial3\"}}, \"process_id\": \"to_scl_dilation_mask\"}}}, \"progress\": 100, \"status\": \"finished\", \"updated\": \"2025-06-03T08:13:14Z\", \"usage\": {\"cpu\": {\"unit\": \"cpu-seconds\", \"value\": 1221.588807721}, \"duration\": {\"unit\": \"seconds\", \"value\": 292}, \"input_pixel\": {\"unit\": \"mega-pixel\", \"value\": 47.66780090332031}, \"max_executor_memory\": {\"unit\": \"gb\", \"value\": 1.9870262145996094}, \"memory\": {\"unit\": \"mb-seconds\", \"value\": 12291644.453645833}, \"network_received\": {\"unit\": \"b\", \"value\": 10879834769}}}}</script>\n",
       "    </openeo-job>\n",
       "    "
      ],
      "text/plain": [
       "<BatchJob job_id='j-2506030807304b9f9c7c0a45c080cc10'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROI_inference_cube = ROI_inference_cube.save_result(format = \"netCDF\")\n",
    "\n",
    "\n",
    "job_ROI = connection.create_job(ROI_inference_cube,\n",
    "    additional=JOB_OPTIONS\n",
    ")\n",
    "\n",
    "job_ROI.start_and_wait()\n",
    "\n",
    "\n",
    "buffer_inference_cube = buffer_inference_cube.save_result(format = \"netCDF\")\n",
    "\n",
    "job_buffer = connection.create_job(buffer_inference_cube,\n",
    "    additional=JOB_OPTIONS\n",
    ")\n",
    "\n",
    "job_buffer.start_and_wait()\n",
    "\n",
    "#TODO I believe the same partitioner is being used, which is not ideal as the artefacts will continue to overlap. Is this caused by the merge_cubes?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "96f01c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Git_projects/WAC/production/prediction/ROI_j-250603080142426893ff124a7609890a/openEO.nc'),\n",
       " WindowsPath('C:/Git_projects/WAC/production/prediction/ROI_j-250603080142426893ff124a7609890a/job-results.json')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import xarray as xr \n",
    "\n",
    "OUTPUT_DIR = \"C:/Git_projects/WAC/production/prediction/ROI_\" + job_ROI.job_id\n",
    "OUTPUT_FILE_ROI = f\"{OUTPUT_DIR}\"\n",
    "job_ROI.get_results().download_files(OUTPUT_FILE_ROI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "123f44f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Git_projects/WAC/production/prediction/buffer_j-2506030807304b9f9c7c0a45c080cc10/openEO.nc'),\n",
       " WindowsPath('C:/Git_projects/WAC/production/prediction/buffer_j-2506030807304b9f9c7c0a45c080cc10/job-results.json')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OUTPUT_DIR = \"C:/Git_projects/WAC/production/prediction/buffer_\" + job_buffer.job_id\n",
    "OUTPUT_FILE_BUFFER = f\"{OUTPUT_DIR}\"\n",
    "job_buffer.get_results().download_files(OUTPUT_FILE_BUFFER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0a1f0a4",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'c:\\\\Git_projects\\\\WAC\\\\openEO\\\\inference\\\\production\\\\prediction\\\\buffer_j-2506020939034c7e91306cda66ba0a9c\\\\openEO.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\file_manager.py:211\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 211\u001b[0m     file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_key\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\lru_cache.py:56\u001b[0m, in \u001b[0;36mLRUCache.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m---> 56\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mmove_to_end(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: [<class 'netCDF4._netCDF4.Dataset'>, ('c:\\\\Git_projects\\\\WAC\\\\openEO\\\\inference\\\\production\\\\prediction\\\\buffer_j-2506020939034c7e91306cda66ba0a9c\\\\openEO.nc',), 'r', (('clobber', True), ('diskless', False), ('format', 'NETCDF4'), ('persist', False)), '46247069-b10e-4f92-8fb6-18c53ecaa9b5']",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m ds_buffer \u001b[38;5;241m=\u001b[39m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mproduction/prediction/buffer_j-2506020939034c7e91306cda66ba0a9c/openEO.nc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m ds_ROI \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproduction/prediction/ROI_j-25060209311144b9ab8c6ecb5c30e973/openEO.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m band_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# or 'unknown_band_1'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\api.py:611\u001b[0m, in \u001b[0;36mopen_dataset\u001b[1;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m    599\u001b[0m decoders \u001b[38;5;241m=\u001b[39m _resolve_decoders_kwargs(\n\u001b[0;32m    600\u001b[0m     decode_cf,\n\u001b[0;32m    601\u001b[0m     open_backend_dataset_parameters\u001b[38;5;241m=\u001b[39mbackend\u001b[38;5;241m.\u001b[39mopen_dataset_parameters,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m     decode_coords\u001b[38;5;241m=\u001b[39mdecode_coords,\n\u001b[0;32m    608\u001b[0m )\n\u001b[0;32m    610\u001b[0m overwrite_encoded_chunks \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite_encoded_chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 611\u001b[0m backend_ds \u001b[38;5;241m=\u001b[39m backend\u001b[38;5;241m.\u001b[39mopen_dataset(\n\u001b[0;32m    612\u001b[0m     filename_or_obj,\n\u001b[0;32m    613\u001b[0m     drop_variables\u001b[38;5;241m=\u001b[39mdrop_variables,\n\u001b[0;32m    614\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdecoders,\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    616\u001b[0m )\n\u001b[0;32m    617\u001b[0m ds \u001b[38;5;241m=\u001b[39m _dataset_from_backend_dataset(\n\u001b[0;32m    618\u001b[0m     backend_ds,\n\u001b[0;32m    619\u001b[0m     filename_or_obj,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    630\u001b[0m )\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:649\u001b[0m, in \u001b[0;36mNetCDF4BackendEntrypoint.open_dataset\u001b[1;34m(self, filename_or_obj, mask_and_scale, decode_times, concat_characters, decode_coords, drop_variables, use_cftime, decode_timedelta, group, mode, format, clobber, diskless, persist, lock, autoclose)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mopen_dataset\u001b[39m(  \u001b[38;5;66;03m# type: ignore[override]  # allow LSP violation, not supporting **kwargs\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    630\u001b[0m     filename_or_obj: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m os\u001b[38;5;241m.\u001b[39mPathLike[Any] \u001b[38;5;241m|\u001b[39m BufferedIOBase \u001b[38;5;241m|\u001b[39m AbstractDataStore,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m     autoclose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    647\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m    648\u001b[0m     filename_or_obj \u001b[38;5;241m=\u001b[39m _normalize_path(filename_or_obj)\n\u001b[1;32m--> 649\u001b[0m     store \u001b[38;5;241m=\u001b[39m \u001b[43mNetCDF4DataStore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    653\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    654\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclobber\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclobber\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    655\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdiskless\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiskless\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    656\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    657\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    658\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    659\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    661\u001b[0m     store_entrypoint \u001b[38;5;241m=\u001b[39m StoreBackendEntrypoint()\n\u001b[0;32m    662\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m close_on_error(store):\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:410\u001b[0m, in \u001b[0;36mNetCDF4DataStore.open\u001b[1;34m(cls, filename, mode, format, group, clobber, diskless, persist, lock, lock_maker, autoclose)\u001b[0m\n\u001b[0;32m    404\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[0;32m    405\u001b[0m     clobber\u001b[38;5;241m=\u001b[39mclobber, diskless\u001b[38;5;241m=\u001b[39mdiskless, persist\u001b[38;5;241m=\u001b[39mpersist, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mformat\u001b[39m\n\u001b[0;32m    406\u001b[0m )\n\u001b[0;32m    407\u001b[0m manager \u001b[38;5;241m=\u001b[39m CachingFileManager(\n\u001b[0;32m    408\u001b[0m     netCDF4\u001b[38;5;241m.\u001b[39mDataset, filename, mode\u001b[38;5;241m=\u001b[39mmode, kwargs\u001b[38;5;241m=\u001b[39mkwargs\n\u001b[0;32m    409\u001b[0m )\n\u001b[1;32m--> 410\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mautoclose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautoclose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:357\u001b[0m, in \u001b[0;36mNetCDF4DataStore.__init__\u001b[1;34m(self, manager, group, mode, lock, autoclose)\u001b[0m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m mode\n\u001b[1;32m--> 357\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mds\u001b[49m\u001b[38;5;241m.\u001b[39mdata_model\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mds\u001b[38;5;241m.\u001b[39mfilepath()\n\u001b[0;32m    359\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_remote \u001b[38;5;241m=\u001b[39m is_remote_uri(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_filename)\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:419\u001b[0m, in \u001b[0;36mNetCDF4DataStore.ds\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mds\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\netCDF4_.py:413\u001b[0m, in \u001b[0;36mNetCDF4DataStore._acquire\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 413\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39macquire_context(needs_lock) \u001b[38;5;28;01mas\u001b[39;00m root:\n\u001b[0;32m    414\u001b[0m         ds \u001b[38;5;241m=\u001b[39m _nc4_require_group(root, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode)\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc\n\u001b[0;32m    134\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerator didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt yield\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\file_manager.py:199\u001b[0m, in \u001b[0;36mCachingFileManager.acquire_context\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;129m@contextlib\u001b[39m\u001b[38;5;241m.\u001b[39mcontextmanager\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21macquire_context\u001b[39m(\u001b[38;5;28mself\u001b[39m, needs_lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    198\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Context manager for acquiring a file.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m     file, cached \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_acquire_with_cache_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mneeds_lock\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\VROMPAYH\\AppData\\Local\\anaconda3\\envs\\wac_env\\lib\\site-packages\\xarray\\backends\\file_manager.py:217\u001b[0m, in \u001b[0;36mCachingFileManager._acquire_with_cache_info\u001b[1;34m(self, needs_lock)\u001b[0m\n\u001b[0;32m    215\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    216\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode\n\u001b[1;32m--> 217\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# ensure file doesn't get overridden when opened again\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2470\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32msrc\\\\netCDF4\\\\_netCDF4.pyx:2107\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'c:\\\\Git_projects\\\\WAC\\\\openEO\\\\inference\\\\production\\\\prediction\\\\buffer_j-2506020939034c7e91306cda66ba0a9c\\\\openEO.nc'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "ds_buffer = xr.open_dataset(\"production/prediction/buffer_j-2506020939034c7e91306cda66ba0a9c/openEO.nc\")\n",
    "ds_ROI = xr.open_dataset(\"production/prediction/ROI_j-25060209311144b9ab8c6ecb5c30e973/openEO.nc\")\n",
    "\n",
    "band_name = 'prediction'  # or 'unknown_band_1'\n",
    "\n",
    "for i, t in enumerate(ds_buffer['t']):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "\n",
    "    # Get the bands for this timestep\n",
    "    band_shift = ds_buffer[band_name].sel(t=t)\n",
    "    band_central = ds_ROI[band_name].sel(t=t)\n",
    "\n",
    "    # Plot the first band (e.g., central) with a greyscale colormap\n",
    "    band_central.plot(ax=ax, cmap='Greys', alpha=0.5, add_colorbar=False)\n",
    "\n",
    "    # Overlay the second band (e.g., shift) with a different colormap or with contours\n",
    "    band_shift.plot(ax=ax, cmap='Reds', alpha=0.5, add_colorbar=False)\n",
    "\n",
    "    ax.set_title(f'{band_name} at {str(t.values)[:10]}')\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
