{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a5bb00",
   "metadata": {},
   "source": [
    "# ComCrop ONNX Model run on OpenEO backend\n",
    "\n",
    "This notebook guides you through running the ComCrop ONNX model using OpenEO.\n",
    "It allows you to interactively select the Area of Interest (AOI) on a map.\n",
    "\n",
    "**Instructions:**\n",
    "1. Ensure `openeo`, `rasterio`, `matplotlib`, `ipyleaflet` and `ipywidgets` are installed.\n",
    "2. Run the 'Imports' cell.\n",
    "3. Run the 'Define Area of Interest (AOI)' cell. A map will appear.\n",
    "4. Use the rectangle drawing tool (on the left side of the map) to draw your desired AOI.\n",
    "5. The coordinates and approximate size (in 10m pixels) will be displayed below the map.\n",
    "   - **Constraint:** The maximum size allowed is 600x600 pixels. If your selection is too large, an error message will appear, and you'll need to draw a smaller rectangle.\n",
    "6. Once a valid AOI is selected, proceed to the subsequent cells to connect to OpenEO and run the processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800fe825",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import openeo\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import ipyleaflet\n",
    "from ipyleaflet import Map, DrawControl, Rectangle, GeoJSON\n",
    "from ipywidgets import Output, VBox, HTML\n",
    "from IPython.display import display\n",
    "import datetime\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore', category=FutureWarning) # Ignore specific FutureWarnings often seen with geo-libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c277071",
   "metadata": {},
   "source": [
    "## Define Area of Interest (AOI)\n",
    "\n",
    "Use the map below to draw a rectangle defining your processing area. The maximum size is approximately 600x600 pixels at 10m resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e9787d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Select AOI on a map ---\n",
    "import math\n",
    "from ipyleaflet import Map, DrawControl\n",
    "from ipywidgets import VBox, HTML, Output\n",
    "import time\n",
    " \n",
    "# --- Configuration ---\n",
    "DEFAULT_RESOLUTION_M = 10\n",
    "INITIAL_ZOOM = 12\n",
    "DEFAULT_CENTER = (4.7, -74.1)  # Bogotá, Colombia\n",
    "MAX_AREA_SQ_M = (600 * DEFAULT_RESOLUTION_M) ** 2\n",
    "MAX_HECTARES = MAX_AREA_SQ_M / 10000  # 3600 ha\n",
    "\n",
    "# --- Widgets ---\n",
    "selection_status = HTML(value=f\"Please draw a rectangle on the map (max area: {MAX_HECTARES:.2f} ha).\")\n",
    "output_widget = Output()\n",
    "\n",
    "# --- State ---\n",
    "selected_extent = {}\n",
    "\n",
    "# --- Map Creation ---\n",
    "m = Map(center=DEFAULT_CENTER, zoom=INITIAL_ZOOM, scroll_wheel_zoom=True)\n",
    "\n",
    "# --- Draw Control ---\n",
    "draw_control = DrawControl(\n",
    "    rectangle={'shapeOptions': {'color': '#0078A8', 'fillOpacity': 0.1}},\n",
    "    polygon={}, circlemarker={}, circle={}, polyline={}, marker={}\n",
    ")\n",
    "m.add_control(draw_control)\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "def calculate_dimensions_and_area(west, south, east, north):\n",
    "    mid_lat = (south + north) / 2\n",
    "    width_m = calculate_distance(mid_lat, west, mid_lat, east)\n",
    "    height_m = calculate_distance(south, west, north, west)\n",
    "    area_sq_m = width_m * height_m\n",
    "    area_ha = area_sq_m / 10000\n",
    "    return width_m, height_m, area_ha\n",
    "\n",
    "# --- Draw Callbacks ---\n",
    "def on_draw_handler(event=None, action=None, geo_json=None, **kwargs):\n",
    "    global selected_extent\n",
    "    with output_widget:\n",
    "        output_widget.clear_output()  # Clear previous output to show only the latest rectangle info\n",
    "        if action == 'created' or action == 'modified':\n",
    "            if geo_json:\n",
    "                coords = geo_json['geometry']['coordinates'][0]\n",
    "                if len(coords) == 5:  # Rectangle has 5 points (closed)\n",
    "                    lons = [p[0] for p in coords]\n",
    "                    lats = [p[1] for p in coords]\n",
    "                    west, east = min(lons), max(lons)\n",
    "                    south, north = min(lats), max(lats)\n",
    "\n",
    "                    width_m, height_m, area_ha = calculate_dimensions_and_area(west, south, east, north)\n",
    "                    width_px = width_m / DEFAULT_RESOLUTION_M\n",
    "                    height_px = height_m / DEFAULT_RESOLUTION_M\n",
    "                    \n",
    "                    if area_ha > MAX_HECTARES:\n",
    "                        selection_status.value = f\"<font color='red'>Error: Area ({area_ha:.2f} ha) exceeds max allowed ({MAX_HECTARES:.2f} ha). Please draw a smaller rectangle.</font>\"\n",
    "                        return\n",
    "                    \n",
    "                    print(f\"Valid rectangle drawn:\")\n",
    "                    print(f\"   Extent: W: {west:.6f}, S: {south:.6f}, E: {east:.6f}, N: {north:.6f}\")\n",
    "                    print(f\"   Size (approx @{DEFAULT_RESOLUTION_M}m): {width_px:.1f} x {height_px:.1f} pixels\")\n",
    "                    print(f\"   Area (approx): {area_ha:.2f} ha\")\n",
    "                    \n",
    "                    selected_extent = {\n",
    "                        'west': west, 'south': south, 'east': east, 'north': north, 'crs': 'EPSG:4326'\n",
    "                    }\n",
    "                    selection_status.value = f\"<font color='green'>Valid AOI selected ({area_ha:.2f} ha). You can now proceed to the next cell.</font>\"\n",
    "        elif action == 'deleted':\n",
    "            selected_extent = {}\n",
    "            selection_status.value = f\"Please draw a rectangle on the map (max area: {MAX_HECTARES:.2f} ha).\"\n",
    "\n",
    "# --- Register Callbacks & Display ---\n",
    "draw_control.on_draw(on_draw_handler)\n",
    "display(VBox([selection_status, m, output_widget]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c131e1e",
   "metadata": {},
   "source": [
    "## Connect to OpenEO Backend, Load Data, Run UDFs and Run ONNX Model on the Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c2bd12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- OpenEO Connection ---\n",
    "print(\"Connecting to openeo.dataspace.copernicus.eu...\")\n",
    "try:\n",
    "    # Direct connection to Copernicus Data Space\n",
    "    connection = openeo.connect(\"openeo.dataspace.copernicus.eu\")\n",
    "    \n",
    "    # Simple authentication using CDSE provider\n",
    "    print(\"Authenticating with CDSE provider...\")\n",
    "    connection.authenticate_oidc(provider_id='CDSE')\n",
    "    print(\"Successfully authenticated.\")\n",
    "    \n",
    "    # Print OpenEO client version for reference\n",
    "    print(f\"OpenEO Client Version: {openeo.__version__}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error connecting or authenticating: {e}\")\n",
    "    connection = None  # Ensure connection is None if failed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f351a2f2-5bf6-4453-8852-428ba01c736f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Load Configuration ---\n",
    "\n",
    "# Ensure connection is established before proceeding\n",
    "if connection is None:\n",
    "    raise ConnectionError(\"Failed to establish OpenEO connection in the previous step.\")\n",
    "\n",
    "# Check if a valid extent was selected\n",
    "if not selected_extent:\n",
    "     raise ValueError(\"No valid Area of Interest (AOI) was selected on the map. Please go back and draw a valid rectangle.\")\n",
    "else:\n",
    "    print(\"Using AOI selected from the map:\")\n",
    "    print(json.dumps(selected_extent, indent=2))\n",
    "    SPATIAL_EXTENT = selected_extent # Use the globally stored extent\n",
    "\n",
    "\n",
    "# URL for model and other dependencies (adjust if needed)\n",
    "model_url = \"https://git.wur.nl/openeo_monitor/comcrop/-/raw/main/openeo_udf/jupyter_example/model.zip\"\n",
    "\n",
    "# Model filename inside the zip\n",
    "model_filename = \"comcrop_udf_test.onnx\" # Make sure this matches the actual ONNX file in model.zip\n",
    "\n",
    "# Path to the model UDF script (relative to notebook or absolute)\n",
    "model_udf_path = \"model_udf_onnx.py\" # Assumes it's in the same directory as the notebook\n",
    "\n",
    "# --- Check if UDF file exists ---\n",
    "if not os.path.exists(model_udf_path):\n",
    "     raise FileNotFoundError(f\"Model UDF script not found at: {model_udf_path}\")\n",
    "else:\n",
    "    print(f\"Model UDF script found: {model_udf_path}\")\n",
    "\n",
    "\n",
    "# --- Define the UDF context ---\n",
    "print(\"Defining the UDF context...\")\n",
    "model_udf_context = {\n",
    "    \"model_path\": f\"model/{model_filename}\", # Path *inside* the OpenEO backend environment after archive extraction\n",
    "}\n",
    "\n",
    "# --- Configure job options ---\n",
    "# Adjust memory based on typical job needs and selected area size\n",
    "# Calculate approximate pixels based on selected extent if needed for dynamic sizing\n",
    "# Ensure width_px and height_px exist in selected_extent\n",
    "total_pixels = selected_extent.get('width_px', 600) * selected_extent.get('height_px', 600)\n",
    "\n",
    "# Example of dynamic memory allocation (but needs refinement based on actual usage)\n",
    "driver_mem = \"4G\" # Usually less needed for driver\n",
    "executor_mem_base = 500 # MB Base memory for executor\n",
    "executor_overhead_base = 4000 # MB Base overhead\n",
    "# Heuristic: Add overhead based on pixel count (experiment: adjust factor as needed)\n",
    "pixels_per_gb_overhead = 1_500_000 # e.g., 1 GB per 1.5M pixels\n",
    "extra_overhead_gb = math.ceil(total_pixels / pixels_per_gb_overhead)\n",
    "executor_overhead = f\"{executor_overhead_base + extra_overhead_gb * 1024}m\"\n",
    "\n",
    "print(f\"Estimated total pixels: {total_pixels:,.0f}\")\n",
    "print(f\"Calculated executor memory overhead: {executor_overhead}\")\n",
    "\n",
    "\n",
    "print(\"Defining the job options...\")\n",
    "job_options = {\n",
    "    \"udf-dependency-archives\": [\n",
    "        f\"{model_url}#model\",  # Fetches zip, extracts to 'model' dir on backend\n",
    "    ],\n",
    "    \"driver-memory\": driver_mem,\n",
    "    \"executor-memory\": f\"{executor_mem_base}m\",\n",
    "    \"executor-memoryOverhead\": executor_overhead,\n",
    "    \"log_level\": \"info\", # Use 'debug' for more verbose UDF logs if needed\n",
    "}\n",
    "print(\"Job Options:\")\n",
    "print(json.dumps(job_options, indent=2))\n",
    "\n",
    "\n",
    "# --- Define Temporal Extent ---\n",
    "# We might want to make this interactive using ipywidgets too (e.g., DatePickers)\n",
    "# For now, using the dates from the original script\n",
    "start_date = \"2023-03-24\"\n",
    "end_date = \"2023-03-24\"\n",
    "TEMPORAL_EXTENT = (start_date, end_date)\n",
    "\n",
    "# SAR requires a wider range for meaningful mean calculation\n",
    "sar_start_date = \"2023-01-01\"\n",
    "sar_end_date = \"2023-12-31\"\n",
    "SAR_TEMPORAL_EXTENT = (sar_start_date, sar_end_date)\n",
    "\n",
    "print(f\"Temporal Extent (Optical): {TEMPORAL_EXTENT}\")\n",
    "print(f\"Temporal Extent (SAR): {SAR_TEMPORAL_EXTENT}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9d6f9e-401f-4753-9c8b-99403cd1d571",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Load UDFs ---\n",
    "\n",
    "# Add coordinates via UDF - Define before using\n",
    "add_coord_udf = openeo.UDF(f\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import math\n",
    "import logging\n",
    "from openeo.udf import XarrayDataCube\n",
    "\n",
    "def apply_datacube(cube: XarrayDataCube, context: dict) -> XarrayDataCube:\n",
    "    data = cube.get_array()\n",
    "\n",
    "    west = {SPATIAL_EXTENT['west']}\n",
    "    south = {SPATIAL_EXTENT['south']}\n",
    "    east = {SPATIAL_EXTENT['east']}\n",
    "    north = {SPATIAL_EXTENT['north']}\n",
    "    \n",
    "    logging.info(\"Coordinate UDF using extent: W:%.6f, S:%.6f, E:%.6f, N:%.6f\" % (west, south, east, north))\n",
    "\n",
    "    # Calculate pixel dimensions\n",
    "    width_pixels = data.x.size\n",
    "    height_pixels = data.y.size\n",
    "    \n",
    "    # Create evenly spaced arrays for longitude and latitude\n",
    "    lon_array = np.linspace(west, east, width_pixels)\n",
    "    lat_array = np.linspace(north, south, height_pixels)  # North to south for correct orientation\n",
    "\n",
    "    # Create 2D meshgrid\n",
    "    lon_grid, lat_grid = np.meshgrid(lon_array, lat_array)\n",
    "\n",
    "    # Debug output\n",
    "    logging.info(\"Extent: %.4f°W, %.4f°S, %.4f°E, %.4f°N\" % (west, south, east, north))\n",
    "    logging.info(\"Grid dimensions: %d×%d pixels\" % (width_pixels, height_pixels))\n",
    "    logging.info(\"Longitude range: %.6f to %.6f\" % (west, east))\n",
    "    logging.info(\"Latitude range: %.6f to %.6f\" % (north, south))\n",
    "\n",
    "    # Create dataset\n",
    "    ds = xr.Dataset({{\n",
    "        \"lon\": ((\"y\", \"x\"), lon_grid),\n",
    "        \"lat\": ((\"y\", \"x\"), lat_grid)\n",
    "    }}).to_array(\"bands\")\n",
    "\n",
    "    return XarrayDataCube(ds)\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "print(\"Add Coordinates UDF defined.\")\n",
    "\n",
    "\n",
    "# Normalisation UDF - Define before using\n",
    "normalise_bands_udf = openeo.UDF(f\"\"\"\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import logging\n",
    "from openeo.udf import XarrayDataCube\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(message)s'  # Simple format showing only the message\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def lnp(message):\n",
    "    '''Log and print the message'''\n",
    "    logger.info(message)\n",
    "\n",
    "def normalise_vv(raster):\n",
    "    raster = np.clip(raster, -25, 0)\n",
    "    return (raster + 25) / 25\n",
    "\n",
    "def normalise_vh(raster):\n",
    "    raster = np.clip(raster, -30, -5)\n",
    "    return (raster + 30) / 25\n",
    "\n",
    "def normalise_longitude(raster):\n",
    "    raster = np.clip(raster, -180, 180)\n",
    "    return (raster + 180) / 360\n",
    "\n",
    "def normalise_latitude(raster):\n",
    "    raster = np.clip(raster, -60, 60)\n",
    "    return (raster + 60) / 120\n",
    "\n",
    "def normalise_altitude(raster):\n",
    "    raster = np.clip(raster, -400, 8000)\n",
    "    return (raster + 400) / 8400\n",
    "\n",
    "def norm(image):\n",
    "    NORM_PERCENTILES = np.array([\n",
    "        [1.7417268007636313, 2.023298706048351],\n",
    "        [1.7261204997060209, 2.038905204308012],\n",
    "        [1.6798346251414997, 2.179592821212937],\n",
    "        [2.3828939530384052, 2.7578332604178284],\n",
    "        [1.7417268007636313, 2.023298706048351],\n",
    "        [1.7417268007636313, 2.023298706048351],\n",
    "        [1.7417268007636313, 2.023298706048351],\n",
    "        [1.7417268007636313, 2.023298706048351],\n",
    "        [1.7417268007636313, 2.023298706048351]\n",
    "    ], dtype=np.float32)\n",
    "    \n",
    "    # Get the shape and ensure we're operating on the right dimensions\n",
    "    if image.ndim == 3 and image.shape[0] == 9:\n",
    "        # Image shape is (9, height, width)\n",
    "        # Reshape percentiles for proper broadcasting: (9,1,1)\n",
    "        min_values = NORM_PERCENTILES[:, 0].reshape(9, 1, 1)\n",
    "        scale_values = NORM_PERCENTILES[:, 1].reshape(9, 1, 1)\n",
    "    else:\n",
    "        # Handle other shapes (original code path)\n",
    "        min_values = NORM_PERCENTILES[:, 0]\n",
    "        scale_values = NORM_PERCENTILES[:, 1]\n",
    "    \n",
    "    # Log the shapes for debugging\n",
    "    # print(f\"Image shape: {{image.shape}}, min_values shape: {{min_values.shape}}\")\n",
    "    \n",
    "    image = np.log(image * 0.005 + 1)\n",
    "    image = (image - min_values) / scale_values\n",
    "    image = np.exp(image * 5 - 1)\n",
    "    return (image / (image + 1)).astype(np.float32)\n",
    "\n",
    "def apply_datacube(cube: XarrayDataCube, context: dict) -> XarrayDataCube:\n",
    "    '''Normalise the input data cube for model inference.\n",
    "    - Normalises the first 9 bands using the norm function\n",
    "    - Normalises VV, VH, DEM, lon, lat separately\n",
    "    - Calculates NDVI from B08 and B04\n",
    "    - Outputs a 15-band cube\n",
    "    '''\n",
    "    original_data = cube.get_array()\n",
    "    lnp(f\"Received data with shape: {{original_data.shape}} and dims: {{original_data.dims}}\")\n",
    "    \n",
    "    # Check if bands are already in the correct order\n",
    "    if list(original_data.coords['bands'].values) != [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"VV\", \"VH\", \"DEM\", \"lon\", \"lat\"]:\n",
    "        # Reorder if necessary\n",
    "        expected_order = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"VV\", \"VH\", \"DEM\", \"lon\", \"lat\"]\n",
    "        lnp(f\"Reordering bands from {{original_data.coords['bands'].values}} to {{expected_order}}\")\n",
    "        x_img = original_data.sel(bands=expected_order)\n",
    "    else:\n",
    "        # Already in correct order\n",
    "        x_img = original_data\n",
    "    \n",
    "    # Get the band names from the cube\n",
    "    band_names = x_img.coords['bands'].values\n",
    "    \n",
    "    # Validate input bands - we need all these bands in this exact order\n",
    "    expected_bands = [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"VV\", \"VH\", \"DEM\", \"lon\", \"lat\"]\n",
    "    for band in expected_bands:\n",
    "        if band not in band_names:\n",
    "            raise ValueError(f\"Missing required band: {{band}}. Available bands: {{band_names}}\")\n",
    "    \n",
    "    # Check if the order matches exactly\n",
    "    if list(band_names) != expected_bands:\n",
    "        # This check is now somewhat redundant due to reordering above, but kept for safety\n",
    "        raise ValueError(f\"Band order mismatch. Expected: {{expected_bands}}, Got: {{list(band_names)}}\")\n",
    "    \n",
    "    # Get the indices for each band by name\n",
    "    try:\n",
    "        band_indices = {{band: list(band_names).index(band) for band in expected_bands}}\n",
    "    except ValueError:\n",
    "        raise ValueError(f\"Could not find all required bands among {{band_names}}\")\n",
    "    \n",
    "    # Convert to numpy for operations\n",
    "    img_values = x_img.values\n",
    "    \n",
    "    # Normalize the first 9 bands (optical) as a single operation\n",
    "    optical_bands = [img_values[band_indices[b]] for b in expected_bands[:9]]\n",
    "    optical_bands_stack = np.stack(optical_bands, axis=0)\n",
    "    normalised_optical = norm(optical_bands_stack)\n",
    "    \n",
    "    # Normalise individual bands\n",
    "    vv_normalised = normalise_vv(img_values[band_indices[\"VV\"]])\n",
    "    vh_normalised = normalise_vh(img_values[band_indices[\"VH\"]])\n",
    "    dem_normalised = normalise_altitude(img_values[band_indices[\"DEM\"]])\n",
    "    lon_normalised = normalise_longitude(img_values[band_indices[\"lon\"]])\n",
    "    lat_normalised = normalise_latitude(img_values[band_indices[\"lat\"]])\n",
    "    \n",
    "    # Calculate NDVI\n",
    "    B08 = img_values[band_indices[\"B08\"]]\n",
    "    B04 = img_values[band_indices[\"B04\"]]\n",
    "    # Avoid division by zero\n",
    "    denominator = B08 + B04\n",
    "    ndvi = np.divide(B08 - B04, denominator, out=np.zeros_like(B08, dtype=np.float32), where=denominator!=0)\n",
    "    ndvi_normalised = np.clip(ndvi, -1, 1) # Standard NDVI range\n",
    "    \n",
    "    # Stack all bands together: 9 optical + NDVI + VV + VH + DEM + Lon + Lat = 15 bands\n",
    "    result_array = np.stack(\n",
    "        [\n",
    "            *normalised_optical,  # Unpack the 9 normalised optical bands\n",
    "            ndvi_normalised,\n",
    "            vv_normalised,\n",
    "            vh_normalised,\n",
    "            dem_normalised,\n",
    "            lon_normalised,\n",
    "            lat_normalised\n",
    "        ],\n",
    "        axis=0\n",
    "    )\n",
    "    \n",
    "    # Create a new list of bands including NDVI - MUST HAVE EXACTLY 15 BANDS FOR MODEL\n",
    "    new_bands = list(band_names[:9]) + ['NDVI'] + list(band_names[9:])\n",
    "    \n",
    "    # Double-check dimensions match\n",
    "    if result_array.shape[0] != len(new_bands):\n",
    "        raise ValueError(f\"Band count mismatch: data shape {{result_array.shape[0]}} != label count {{len(new_bands)}}\")    \n",
    "    \n",
    "    # Create output DataArray with the same coordinates but normalised values\n",
    "    # Explicitly use (bands, y, x) order for GeoTrellis compatibility\n",
    "    output_data = xr.DataArray(\n",
    "        result_array,\n",
    "        dims=['bands', 'y', 'x'],\n",
    "        coords={{'bands': new_bands, 'y': x_img.coords['y'], 'x': x_img.coords['x']}}\n",
    "    )\n",
    "    \n",
    "    lnp(f\"Final output dimensions: {{output_data.dims}} with shape {{output_data.shape}}\") \n",
    "    lnp(f\"Final bands: {{output_data.coords['bands'].values}}\") \n",
    "    \n",
    "    return XarrayDataCube(output_data)\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "print(\"Normalisation UDF defined.\")\n",
    "\n",
    "\n",
    "# Load the Model UDF from file\n",
    "print(f\"Loading Model UDF from file: {model_udf_path}...\")\n",
    "\n",
    "# Pass the context defined earlier (contains model_path for backend)\n",
    "model_udf = openeo.UDF.from_file(model_udf_path, context=model_udf_context)\n",
    "print(\"Model UDF loaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3598c166-c0f0-4927-a6c0-3e60f87c8d87",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Load Collections ---\n",
    "\n",
    "# Check extent and connection again before loading\n",
    "if not selected_extent or connection is None:\n",
    "    raise ValueError(\"Cannot load data: OpenEO connection or spatial extent is missing.\")\n",
    "\n",
    "print(\"Loading Sentinel-1 collection (Global Mosaics)...\")\n",
    "# Load Data Collections\n",
    "print(\"Loading Sentinel-1 collection...\")\n",
    "sentinel1 = connection.load_collection(\n",
    "    \"SENTINEL1_GLOBAL_MOSAICS\",\n",
    "    spatial_extent=SPATIAL_EXTENT,\n",
    "    temporal_extent=SAR_TEMPORAL_EXTENT,\n",
    "    bands=[\"VV\", \"VH\"]\n",
    ")\n",
    "sar_cube = sentinel1.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "\n",
    "# Apply a simple dB conversion to the result\n",
    "# Note: This is a simple log transform that assumes the data is in linear scale\n",
    "print(\"Converting SAR data to dB scale (post time-reduction)...\")\n",
    "sar_cube = sar_cube.apply(lambda x: 10 * x.log(base=10))\n",
    "print(\"SAR data converted to dB scale.\")\n",
    "\n",
    "print(\"Loading Sentinel-2 collection...\")\n",
    "sentinel2 = connection.load_collection(\n",
    "    \"SENTINEL2_L2A\",\n",
    "    temporal_extent=TEMPORAL_EXTENT,\n",
    "    spatial_extent=SPATIAL_EXTENT,\n",
    "    bands=[\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\"]\n",
    ")\n",
    "\n",
    "# Get RGB bands for resampling\n",
    "sentinel2_RGB = sentinel2.filter_bands([\"B04\", \"B03\", \"B02\"])\n",
    "\n",
    "print(\"Loading DEM collection...\")\n",
    "dem = connection.load_collection(\n",
    "    \"COPERNICUS_30\",\n",
    "    spatial_extent=SPATIAL_EXTENT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2c2164-41aa-4340-a71a-550ef3a3f81e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Resampling and Prepairing Collections ---\n",
    "print(\"Resampling and preparing collections...\")\n",
    "# Reduce time dimension for all collections to eliminate temporal dimension issues\n",
    "sentinel2_reduced = sentinel2.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "print(\"Sentinel-2 time dimension reduced.\")\n",
    "\n",
    "# Keep Sentinel-2 resampling to ensure all bands are properly aligned\n",
    "sentinel2_resampled = sentinel2_reduced.resample_cube_spatial(sentinel2_RGB)\n",
    "# Don't resample SAR explicitly - let merge_cubes handle it\n",
    "sar_resampled = sar_cube.resample_cube_spatial(sentinel2_RGB, method=\"near\")\n",
    "# DEM needs bilinear resampling specifically\n",
    "dem_resampled = dem.resample_cube_spatial(sentinel2_RGB, method=\"bilinear\")\n",
    "\n",
    "# Attempt to reduce time dimension for DEM (will be a no-op if dimension doesn't exist)\n",
    "try:\n",
    "    dem_resampled = dem_resampled.reduce_dimension(dimension=\"t\", reducer=\"mean\")\n",
    "    print(\"DEM time dimension reduced.\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: DEM doesn't have a time dimension or couldn't be reduced: {str(e)}\")\n",
    "\n",
    "print(\"Applying add-coordinate UDF...\")\n",
    "# Apply the coordinate UDF to SAR data with consistent resampling to ensure proper alignment\n",
    "# Important: We need consistent resampling to avoid grid bounds errors\n",
    "sar_resampled = sar_cube.resample_cube_spatial(sentinel2_RGB, method=\"near\")\n",
    "sentinel1_coords = sar_resampled.apply(process=add_coord_udf)\n",
    "sentinel1_coords_resampled = sentinel1_coords.rename_labels(\"bands\", [\"lon\", \"lat\"])\n",
    "\n",
    "print(\"Collections resampled and prepared.\")\n",
    "\n",
    "# Merge data cubes\n",
    "print(\"Merging data cubes...\")\n",
    "merged_datacube = (\n",
    "    sentinel2_resampled\n",
    "    .merge_cubes(sar_resampled)\n",
    "    .merge_cubes(dem_resampled)\n",
    "    .merge_cubes(sentinel1_coords_resampled)\n",
    ")\n",
    "\n",
    "\n",
    "# Preparing bands for model processing\n",
    "print(\"Preparing bands for model processing...\")\n",
    "existing_band_labels = merged_datacube.dimension_labels('bands')\n",
    "print(f\"Original band labels: {existing_band_labels}\")\n",
    "\n",
    "# Rename bands to exactly match the expected order for the UDF\n",
    "# No NDVI yet - it will be added by the normalization UDF\n",
    "cubemerged = merged_datacube.rename_labels(\n",
    "    'bands', [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"VV\", \"VH\", \"DEM\", \"lon\", \"lat\"]\n",
    ")\n",
    "print(f\"Renamed band labels: {cubemerged.dimension_labels('bands')}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c55a368-669f-4409-a522-801d2415d24d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Apply Normalisation UDF ---\n",
    "print(\"Applying normalisation UDF...\")\n",
    "cubemerged = cubemerged.apply(process=normalise_bands_udf)\n",
    "\n",
    "# After normalisation, we need to explicitly update band labels to include NDVI\n",
    "# This ensures the dimension labels match the actual 15 bands output by the UDF\n",
    "print(\"Updating band labels to include NDVI...\")\n",
    "cubemerged = cubemerged.rename_labels(\n",
    "    'bands', [\"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B11\", \"B12\", \"NDVI\", \"VV\", \"VH\", \"DEM\", \"lon\", \"lat\"]\n",
    ")\n",
    "print(f\"Final band labels: {cubemerged.dimension_labels('bands')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5df62-b6d7-499c-b669-a52cc619915a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Run Model UDF with Neighborhood Processing ---\n",
    "\n",
    "print(\"Proceeding to apply_neighborhood...\")\n",
    "\n",
    "# Use smaller patches with overlap to prevent grid bounds issues\n",
    "print(\"Running model UDF with neighborhood processing...\")\n",
    "patch_size = 48  # Reduced from 64 to avoid grid bounds issues\n",
    "overlap = 8     # Add overlap to handle edge cases\n",
    "print(f\"Using patch size {patch_size}x{patch_size} with {overlap} pixel overlap\")\n",
    "print(\"Monitor batch job @ https://openeo.dataspace.copernicus.eu/\")\n",
    "\n",
    "result = cubemerged.apply_neighborhood(\n",
    "    process=model_udf,\n",
    "    size=[\n",
    "        {'dimension': 'x', 'value': patch_size, 'unit': 'px'},\n",
    "        {'dimension': 'y', 'value': patch_size, 'unit': 'px'}\n",
    "    ],\n",
    "    overlap=[\n",
    "        {'dimension': 'x', 'value': overlap, 'unit': 'px'},\n",
    "        {'dimension': 'y', 'value': overlap, 'unit': 'px'}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Set up the output file path\n",
    "target_file = f\"{model_filename}~jn~prediction.tif\"\n",
    "\n",
    "# Create and start a batch job\n",
    "main_job = result.execute_batch(target_file, \n",
    "    job_options=job_options, \n",
    "    title=f\"{model_filename}~jn~ONNX Prediction\"\n",
    ")\n",
    "print(f\"Main batch job {main_job.job_id} finished. Output will be saved to {target_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f7b87d-c4c6-4208-a3af-f91428eef3fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# --- Visualise Results ---\n",
    "if main_job.status() == 'finished':\n",
    "    print(\"If job completed successfully, we can show results...\")\n",
    "    # Download results to the directory where the notebook is running\n",
    "    results = main_job.get_results()\n",
    "    results.download_files()\n",
    "    print(f\"Results (probably) downloaded. Check for file... {target_file}\")\n",
    "    \n",
    "    \n",
    "    # Now try to open and display the file\n",
    "    print(f\"Opening result file: {target_file}\")\n",
    "    with rasterio.open(target_file) as dataset:\n",
    "        print(f\"Dataset properties:\")\n",
    "        print(f\"  Driver: {dataset.driver}\")\n",
    "        print(f\"  CRS: {dataset.crs}\")\n",
    "        print(f\"  Count: {dataset.count}\")\n",
    "        print(f\"  Width: {dataset.width}, Height: {dataset.height}\")\n",
    "        print(f\"  Bounds: {dataset.bounds}\")\n",
    "\n",
    "        # Display the first band\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "        show(dataset.read(1), ax=ax, cmap='viridis', title=target_file)\n",
    "        plt.show()\n",
    "else:\n",
    "    print(f\"Showing results failed... Status: {main_job.status()}\")\n",
    "    print(\"Please check the job logs above or on the OpenEO platform for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3087f5a5-f4d5-4060-9363-37b985ec4b2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wac_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
