{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EO Data Extraction Workflow\n",
    "This notebook demonstrates a streamlined workflow for extracting and processing Earth Observation (EO) data \n",
    "using the **openEO** Python client. \n",
    "\n",
    "### Key Steps:\n",
    "1. Load and align input data (shapefile).\n",
    "2. Generate UTM-aligned patches for analysis.\n",
    "3. Split patches into smaller manageable jobs.\n",
    "4. Run the extraction process using openEO backends.\n",
    "5. View and analyze the outputs (e.g., NetCDF files).\n",
    "\n",
    "### Required Libraries:\n",
    "- `openeo` for interacting with EO backends.\n",
    "- `openeo-gfmap` for handling geospatial data.\n",
    "\n",
    "### Step 1: load in the shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openeo\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "\n",
    "# Import functions from the cleaned-up module\n",
    "from helper.eo_utils import (\n",
    "    generate_patches_by_crs,\n",
    "    process_split_jobs,\n",
    "    create_job_dataframe\n",
    ")\n",
    "\n",
    "# Filepath to the input shapefile\n",
    "file_path = Path(r\"C:\\Git_projects\\WAC\\production\\resources\\Land_use_Roads_tile.shp\")\n",
    "\n",
    "# Load input shapefile as GeoDataFrame\n",
    "base_df = gpd.read_file(file_path)\n",
    "print(base_df.head())  # Preview the input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Generate Patches\n",
    "\n",
    "Here we create as many UTM-aligned patches as we can, within the provided polygon, based on the following parameters:\n",
    "- **Patch size**: Size of each patch in pixels.\n",
    "- **Resolution**: Alignment resolution in meters.\n",
    "\n",
    "To complete the dataframe required for the job manager we add in the temporal extent as well:\n",
    "- **Start date and duration**: Temporal extent for data extraction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for processing\n",
    "patch_size = 64          # Size of patches in pixels\n",
    "resolution = 10.0         # Alignment resolution in meters\n",
    "start_date = \"2023-01-01\" # Temporal extent start date\n",
    "nb_months = 3             # Number of months for the temporal extent\n",
    "\n",
    "# Step 1: Generate aligned patches by UTM CRS\n",
    "dataframes_by_crs = generate_patches_by_crs(\n",
    "    base_gdf=base_df,\n",
    "    start_date=start_date,\n",
    "    duration_months=nb_months,\n",
    "    patch_size=patch_size,\n",
    "    resolution=resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Patches into Jobs\n",
    "We combine the patches into jobs using an **S2 tile grid system**. Such combination allows us to extract multiple patches within one openEO batch job, thereby reducing the total cost\n",
    "\n",
    "Parameters include:\n",
    "- **Max points per job**: Controls the size of each job.\n",
    "- **H3 resolution**: Sets the grid resolution for spatial division."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_points = 10           # Maximum points per job for splitting\n",
    "grid_resolution = 3       # H3 index resolution\n",
    "\n",
    "# Step 2: Process the patches into split jobs with H3 indexing\n",
    "split_jobs = process_split_jobs(\n",
    "    geodataframes=dataframes_by_crs,\n",
    "    max_points=max_points,\n",
    "    grid_resolution=grid_resolution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Job DataFrame\n",
    "From the splitted jobs we create a dataframe which we can use for the MultiBackendJobManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a summary DataFrame for the split jobs\n",
    "job_dataframe = create_job_dataframe(split_jobs)\n",
    "job_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For testing; reduce the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dataframe = job_dataframe[0:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit Extraction Jobs\n",
    "\n",
    "Using the openEO backend, we authenticate and submit the jobs to process the EO data. \n",
    "Each job extracts Sentinel and climate data for its assigned spatial and temporal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Authenticate and connect to openEO backend\n",
    "connection = openeo.connect(url=\"openeo.dataspace.copernicus.eu\").authenticate_oidc()\n",
    "\n",
    "# Initialize MultiBackendJobManager\n",
    "from openeo.extra.job_management import MultiBackendJobManager, ParquetJobDatabase\n",
    "\n",
    "manager = MultiBackendJobManager()\n",
    "manager.add_backend(\"cdse\", connection=connection, parallel_jobs=2)\n",
    "\n",
    "# Initialize or load job tracker\n",
    "job_tracker = 'job_tracker.parquet'\n",
    "job_db = ParquetJobDatabase(path=job_tracker)\n",
    "\n",
    "if not job_db.exists():\n",
    "    df = manager._normalize_df(job_dataframe)\n",
    "    job_db.persist(df)\n",
    "\n",
    "# Submit jobs\n",
    "from eo_extractors.extractor import wac_extraction_job\n",
    "manager.run_jobs(start_job=wac_extraction_job, job_db=job_db)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
